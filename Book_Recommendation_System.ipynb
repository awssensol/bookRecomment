{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/awssensol/bookRecomment/blob/main/Copy_of_Book_Recommendation_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T19:11:17.056382Z",
     "iopub.status.busy": "2025-06-26T19:11:17.056091Z",
     "iopub.status.idle": "2025-06-26T19:11:19.743180Z",
     "shell.execute_reply": "2025-06-26T19:11:19.742275Z",
     "shell.execute_reply.started": "2025-06-26T19:11:17.056350Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in /opt/conda/lib/python3.12/site-packages (1.7.4.5)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.12/site-packages (from kaggle) (6.2.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /opt/conda/lib/python3.12/site-packages (from kaggle) (2025.4.26)\n",
      "Requirement already satisfied: charset-normalizer in /opt/conda/lib/python3.12/site-packages (from kaggle) (3.4.2)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.12/site-packages (from kaggle) (3.10)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.12/site-packages (from kaggle) (5.28.3)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/conda/lib/python3.12/site-packages (from kaggle) (2.9.0.post0)\n",
      "Requirement already satisfied: python-slugify in /opt/conda/lib/python3.12/site-packages (from kaggle) (8.0.4)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from kaggle) (2.32.3)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /opt/conda/lib/python3.12/site-packages (from kaggle) (80.1.0)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.12/site-packages (from kaggle) (1.17.0)\n",
      "Requirement already satisfied: text-unidecode in /opt/conda/lib/python3.12/site-packages (from kaggle) (1.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from kaggle) (4.67.1)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in /opt/conda/lib/python3.12/site-packages (from kaggle) (1.26.19)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.12/site-packages (from kaggle) (0.5.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T19:11:19.744364Z",
     "iopub.status.busy": "2025-06-26T19:11:19.744117Z",
     "iopub.status.idle": "2025-06-26T19:11:21.463041Z",
     "shell.execute_reply": "2025-06-26T19:11:21.462082Z",
     "shell.execute_reply.started": "2025-06-26T19:11:19.744338Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub in /opt/conda/lib/python3.12/site-packages (0.3.12)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from kagglehub) (24.2)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.12/site-packages (from kagglehub) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from kagglehub) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from kagglehub) (4.67.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->kagglehub) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->kagglehub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->kagglehub) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->kagglehub) (2025.4.26)\n"
     ]
    }
   ],
   "source": [
    "!pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-06-26T19:11:21.465511Z",
     "iopub.status.busy": "2025-06-26T19:11:21.464407Z",
     "iopub.status.idle": "2025-06-26T19:11:22.014981Z",
     "shell.execute_reply": "2025-06-26T19:11:22.014239Z",
     "shell.execute_reply.started": "2025-06-26T19:11:21.465473Z"
    },
    "id": "9mi_w1ImXwlS",
    "outputId": "851a9cf8-ec43-4587-bf0d-9a90ca8d9426"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/sagemaker-user/.cache/kagglehub/datasets/mohamedbakhet/amazon-books-reviews/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"mohamedbakhet/amazon-books-reviews\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-06-26T19:11:22.016449Z",
     "iopub.status.busy": "2025-06-26T19:11:22.015958Z",
     "iopub.status.idle": "2025-06-26T19:11:38.623327Z",
     "shell.execute_reply": "2025-06-26T19:11:38.622463Z",
     "shell.execute_reply.started": "2025-06-26T19:11:22.016417Z"
    },
    "id": "PoHTvx3cX2gM",
    "outputId": "317a2078-41cb-495c-ffda-044f461e9e1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.12/site-packages (1.37.1)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.12/site-packages (4.51.3)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: keras in /opt/conda/lib/python3.12/site-packages (3.9.2)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.12/site-packages (1.6.1)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: botocore<1.38.0,>=1.37.1 in /opt/conda/lib/python3.12/site-packages (from boto3) (1.37.1)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.12/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /opt/conda/lib/python3.12/site-packages (from boto3) (0.11.3)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.12/site-packages (from botocore<1.38.0,>=1.37.1->boto3) (1.26.19)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch) (80.1.0)\n",
      "Requirement already satisfied: sympy!=1.13.2,>=1.13.1 in /opt/conda/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.12/site-packages (from keras) (2.2.0)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.12/site-packages (from keras) (14.0.0)\n",
      "Requirement already satisfied: namex in /opt/conda/lib/python3.12/site-packages (from keras) (0.0.9)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.12/site-packages (from keras) (3.13.0)\n",
      "Requirement already satisfied: optree in /opt/conda/lib/python3.12/site-packages (from keras) (0.15.0)\n",
      "Requirement already satisfied: ml-dtypes in /opt/conda/lib/python3.12/site-packages (from keras) (0.4.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.12/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy!=1.13.2,>=1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 19:11:34.429832: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[nltk_data] Downloading package stopwords to /home/sagemaker-\n",
      "[nltk_data]     user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install pandas boto3 transformers torch keras scikit-learn nltk\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import boto3\n",
    "import nltk\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-06-26T19:11:38.627317Z",
     "iopub.status.busy": "2025-06-26T19:11:38.625624Z",
     "iopub.status.idle": "2025-06-26T19:11:40.454891Z",
     "shell.execute_reply": "2025-06-26T19:11:40.454118Z",
     "shell.execute_reply.started": "2025-06-26T19:11:38.627255Z"
    },
    "id": "AxvLr3hDcINS",
    "outputId": "e7f43cb7-64ef-4305-9dc1-0e18ff6bd968"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: swifter in /opt/conda/lib/python3.12/site-packages (1.4.0)\n",
      "Requirement already satisfied: pandas>=1.0.0 in /opt/conda/lib/python3.12/site-packages (from swifter) (2.2.3)\n",
      "Requirement already satisfied: psutil>=5.6.6 in /opt/conda/lib/python3.12/site-packages (from swifter) (5.9.8)\n",
      "Requirement already satisfied: dask>=2.10.0 in /opt/conda/lib/python3.12/site-packages (from dask[dataframe]>=2.10.0->swifter) (2025.4.1)\n",
      "Requirement already satisfied: tqdm>=4.33.0 in /opt/conda/lib/python3.12/site-packages (from swifter) (4.67.1)\n",
      "Requirement already satisfied: click>=8.1 in /opt/conda/lib/python3.12/site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (8.1.8)\n",
      "Requirement already satisfied: cloudpickle>=3.0.0 in /opt/conda/lib/python3.12/site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (3.1.1)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /opt/conda/lib/python3.12/site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (24.2)\n",
      "Requirement already satisfied: partd>=1.4.0 in /opt/conda/lib/python3.12/site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (1.4.2)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /opt/conda/lib/python3.12/site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (6.0.2)\n",
      "Requirement already satisfied: toolz>=0.10.0 in /opt/conda/lib/python3.12/site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (0.12.1)\n",
      "Requirement already satisfied: pyarrow>=14.0.1 in /opt/conda/lib/python3.12/site-packages (from dask[dataframe]>=2.10.0->swifter) (19.0.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from pandas>=1.0.0->swifter) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas>=1.0.0->swifter) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas>=1.0.0->swifter) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas>=1.0.0->swifter) (2025.2)\n",
      "Requirement already satisfied: locket in /opt/conda/lib/python3.12/site-packages (from partd>=1.4.0->dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (1.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->swifter) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-26T19:12:13.066Z",
     "iopub.execute_input": "2025-06-26T19:11:40.456127Z",
     "iopub.status.busy": "2025-06-26T19:11:40.455877Z"
    },
    "id": "yh8FexfEbgq1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews columns: ['Id', 'Title', 'Price', 'User_id', 'profileName', 'review/helpfulness', 'review/score', 'review/time', 'review/summary', 'review/text']\n",
      "Books columns: ['Title', 'description', 'authors', 'image', 'previewLink', 'publisher', 'publishedDate', 'infoLink', 'categories', 'ratingsCount']\n",
      "Reviews columns: ['Id', 'Title', 'Price', 'User_id', 'profileName', 'review/helpfulness', 'review/score', 'review/time', 'review/summary', 'review/text']\n",
      "Books columns: ['Title', 'description', 'authors', 'image', 'previewLink', 'publisher', 'publishedDate', 'infoLink', 'categories', 'ratingsCount']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "import swifter\n",
    "from functools import lru_cache\n",
    "\n",
    "class ReviewProcessor:\n",
    "    SENTIMENT_MAPPING = {\n",
    "        1: 0,  # Negative\n",
    "        2: 0,  # Negative\n",
    "        3: 1,  # Neutral\n",
    "        4: 2,  # Positive\n",
    "        5: 2   # Positive\n",
    "    }\n",
    "\n",
    "    def __init__(self, reviews_path: str, books_path: str):\n",
    "        self.reviews_path = Path(reviews_path)\n",
    "        self.books_path = Path(books_path)\n",
    "        self.reviews_df = None\n",
    "        self.books_df = None\n",
    "        self.merged_df = None\n",
    "\n",
    "        # Compile regex patterns once\n",
    "        self.punctuation_pattern = re.compile(r'[^a-zA-Z\\s]')\n",
    "        self.spaces_pattern = re.compile(r'\\s+')\n",
    "\n",
    "        # Verify files exist\n",
    "        if not self.reviews_path.exists():\n",
    "            raise FileNotFoundError(f\"Reviews file not found: {reviews_path}\")\n",
    "        if not self.books_path.exists():\n",
    "            raise FileNotFoundError(f\"Books file not found: {books_path}\")\n",
    "\n",
    "    def inspect_files(self):\n",
    "        \"\"\"\n",
    "        Print the column names of both CSV files\n",
    "        \"\"\"\n",
    "        # Read just the headers\n",
    "        reviews_cols = pd.read_csv(self.reviews_path, nrows=0).columns\n",
    "        books_cols = pd.read_csv(self.books_path, nrows=0).columns\n",
    "\n",
    "        print(\"Reviews columns:\", list(reviews_cols))\n",
    "        print(\"Books columns:\", list(books_cols))\n",
    "        return reviews_cols, books_cols\n",
    "\n",
    "    @lru_cache(maxsize=10000)\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "\n",
    "        text = str(text).lower()\n",
    "        text = self.punctuation_pattern.sub('', text)\n",
    "        text = self.spaces_pattern.sub(' ', text)\n",
    "        return text.strip()\n",
    "\n",
    "    def load_data(self) -> None:\n",
    "        # First inspect the columns\n",
    "        reviews_cols, books_cols = self.inspect_files()\n",
    "\n",
    "        dtypes = {\n",
    "            'reviewerID': 'category',\n",
    "            'asin': 'category',\n",
    "            'overall': 'float32'\n",
    "        }\n",
    "\n",
    "        # Read the files with the actual column names\n",
    "        self.reviews_df = pd.read_csv(self.reviews_path)\n",
    "        self.books_df = pd.read_csv(self.books_path)\n",
    "\n",
    "        # Print shape information\n",
    "        print(f\"Reviews shape: {self.reviews_df.shape}\")\n",
    "        print(f\"Books shape: {self.books_df.shape}\")\n",
    "\n",
    "    def process_reviews(self) -> None:\n",
    "        if 'reviewText' in self.reviews_df.columns:\n",
    "            review_col = 'reviewText'\n",
    "        else:\n",
    "            # Try to find a similar column name or use the first text column\n",
    "            text_cols = self.reviews_df.select_dtypes(include=['object']).columns\n",
    "            review_col = text_cols[0] if len(text_cols) > 0 else None\n",
    "\n",
    "        if review_col is None:\n",
    "            raise ValueError(\"Could not find review text column\")\n",
    "\n",
    "        self.reviews_df['cleaned_review'] = (\n",
    "            self.reviews_df[review_col]\n",
    "            .swifter.apply(self.clean_text)\n",
    "        )\n",
    "\n",
    "        if 'overall' in self.reviews_df.columns:\n",
    "            self.reviews_df['label'] = self.reviews_df['overall'].map(self.SENTIMENT_MAPPING)\n",
    "\n",
    "    def merge_data(self) -> pd.DataFrame:\n",
    "        # Identify common columns for merging\n",
    "        common_cols = set(self.reviews_df.columns) & set(self.books_df.columns)\n",
    "        if not common_cols:\n",
    "            raise ValueError(\"No common columns found for merging datasets\")\n",
    "\n",
    "        merge_col = list(common_cols)[0]  # Use the first common column\n",
    "        print(f\"Merging on column: {merge_col}\")\n",
    "\n",
    "        self.merged_df = pd.merge(\n",
    "            self.reviews_df,\n",
    "            self.books_df,\n",
    "            on=merge_col,\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "        self.optimize_memory()\n",
    "        return self.merged_df\n",
    "\n",
    "    def optimize_memory(self) -> None:\n",
    "        for df in [self.reviews_df, self.books_df, self.merged_df]:\n",
    "            if df is not None:\n",
    "                for col in df.select_dtypes(include=['object']).columns:\n",
    "                    if df[col].nunique() / len(df) < 0.5:\n",
    "                        df[col] = df[col].astype('category')\n",
    "\n",
    "    def process(self) -> pd.DataFrame:\n",
    "        self.load_data()\n",
    "        self.process_reviews()\n",
    "        return self.merge_data()\n",
    "\n",
    "\n",
    "# Define the file paths\n",
    "reviews_file_path = \"/home/sagemaker-user/.cache/kagglehub/datasets/mohamedbakhet/amazon-books-reviews/versions/1/Books_rating.csv\"\n",
    "books_details_file_path = \"/home/sagemaker-user/.cache/kagglehub/datasets/mohamedbakhet/amazon-books-reviews/versions/1/books_data.csv\"\n",
    "\n",
    "# Usage\n",
    "processor = ReviewProcessor(reviews_file_path, books_details_file_path)\n",
    "# First inspect the files\n",
    "processor.inspect_files()\n",
    "# Then process\n",
    "result_df = processor.process()\n",
    "\n",
    "# Display the first few rows of the result\n",
    "print(\"\\nFirst few rows of processed data:\")\n",
    "display(result_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-26T19:12:13.067Z"
    },
    "id": "1HW8uaHkciL1"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "X_train, X_val, y_train, y_val = train_test_split(df[\"cleaned_review\"], df[\"label\"], test_size=0.2)\n",
    "\n",
    "train_encodings = tokenizer(list(X_train), truncation=True, padding=True)\n",
    "val_encodings = tokenizer(list(X_val), truncation=True, padding=True)\n",
    "\n",
    "class ReviewDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}, torch.tensor(self.labels[idx])\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = ReviewDataset(train_encodings, y_train.tolist())\n",
    "val_dataset = ReviewDataset(val_encodings, y_val.tolist())\n",
    "\n",
    "bert_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    logging_dir=\"./bert_logs\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(model=bert_model, args=training_args, train_dataset=train_dataset, eval_dataset=val_dataset)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-26T19:12:13.067Z"
    },
    "id": "WRdlV7D7cwhl"
   },
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "import swifter\n",
    "from functools import lru_cache\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# First, define and run the ReviewProcessor\n",
    "class ReviewProcessor:\n",
    "    SENTIMENT_MAPPING = {\n",
    "        1: 0,  # Negative\n",
    "        2: 0,  # Negative\n",
    "        3: 1,  # Neutral\n",
    "        4: 2,  # Positive\n",
    "        5: 2   # Positive\n",
    "    }\n",
    "\n",
    "    def __init__(self, reviews_path: str, books_path: str):\n",
    "        self.reviews_path = Path(reviews_path)\n",
    "        self.books_path = Path(books_path)\n",
    "        self.reviews_df = None\n",
    "        self.books_df = None\n",
    "        self.merged_df = None\n",
    "\n",
    "        # Compile regex patterns once\n",
    "        self.punctuation_pattern = re.compile(r'[^a-zA-Z\\s]')\n",
    "        self.spaces_pattern = re.compile(r'\\s+')\n",
    "\n",
    "        # Verify files exist\n",
    "        if not self.reviews_path.exists():\n",
    "            raise FileNotFoundError(f\"Reviews file not found: {reviews_path}\")\n",
    "        if not self.books_path.exists():\n",
    "            raise FileNotFoundError(f\"Books file not found: {books_path}\")\n",
    "\n",
    "    @lru_cache(maxsize=10000)\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "\n",
    "        text = str(text).lower()\n",
    "        text = self.punctuation_pattern.sub('', text)\n",
    "        text = self.spaces_pattern.sub(' ', text)\n",
    "        return text.strip()\n",
    "\n",
    "    def load_data(self) -> None:\n",
    "        self.reviews_df = pd.read_csv(self.reviews_path)\n",
    "        self.books_df = pd.read_csv(self.books_path)\n",
    "\n",
    "        print(f\"Reviews shape: {self.reviews_df.shape}\")\n",
    "        print(f\"Books shape: {self.books_df.shape}\")\n",
    "\n",
    "    def process_reviews(self) -> None:\n",
    "        if 'reviewText' in self.reviews_df.columns:\n",
    "            review_col = 'reviewText'\n",
    "        else:\n",
    "            text_cols = self.reviews_df.select_dtypes(include=['object']).columns\n",
    "            review_col = text_cols[0] if len(text_cols) > 0 else None\n",
    "\n",
    "        if review_col is None:\n",
    "            raise ValueError(\"Could not find review text column\")\n",
    "\n",
    "        self.reviews_df['cleaned_review'] = (\n",
    "            self.reviews_df[review_col]\n",
    "            .apply(self.clean_text)  # Removed swifter for simplicity\n",
    "        )\n",
    "\n",
    "        if 'overall' in self.reviews_df.columns:\n",
    "            self.reviews_df['label'] = self.reviews_df['overall'].map(self.SENTIMENT_MAPPING)\n",
    "\n",
    "    def merge_data(self) -> pd.DataFrame:\n",
    "        common_cols = set(self.reviews_df.columns) & set(self.books_df.columns)\n",
    "        if not common_cols:\n",
    "            raise ValueError(\"No common columns found for merging datasets\")\n",
    "\n",
    "        merge_col = list(common_cols)[0]\n",
    "        print(f\"Merging on column: {merge_col}\")\n",
    "\n",
    "        self.merged_df = pd.merge(\n",
    "            self.reviews_df,\n",
    "            self.books_df,\n",
    "            on=merge_col,\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "        return self.merged_df\n",
    "\n",
    "    def process(self) -> pd.DataFrame:\n",
    "        self.load_data()\n",
    "        self.process_reviews()\n",
    "        return self.merge_data()\n",
    "\n",
    "# Process the data\n",
    "reviews_file_path = \"/kaggle/input/amazon-books-reviews/Books_rating.csv\"\n",
    "books_details_file_path = \"/kaggle/input/amazon-books-reviews/books_data.csv\"\n",
    "\n",
    "processor = ReviewProcessor(reviews_file_path, books_details_file_path)\n",
    "df = processor.process()\n",
    "\n",
    "# Optional: Use a smaller subset of data if needed\n",
    "# df = df.sample(n=10000, random_state=42)\n",
    "\n",
    "print(\"\\nDataFrame shape:\", df.shape)\n",
    "print(\"\\nColumns:\", df.columns.tolist())\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "# LSTM model parameters\n",
    "max_words = 5000\n",
    "max_len = 300\n",
    "\n",
    "# Tokenization\n",
    "tokenizer_lstm = Tokenizer(num_words=max_words)\n",
    "tokenizer_lstm.fit_on_texts(df[\"cleaned_review\"])\n",
    "X_lstm = tokenizer_lstm.texts_to_sequences(df[\"cleaned_review\"])\n",
    "X_lstm = pad_sequences(X_lstm, maxlen=max_len)\n",
    "y_lstm = to_categorical(df[\"label\"])\n",
    "\n",
    "# Create LSTM model\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(Embedding(max_words, 128, input_length=max_len))\n",
    "lstm_model.add(LSTM(64))\n",
    "lstm_model.add(Dropout(0.3))\n",
    "lstm_model.add(Dense(3, activation=\"softmax\"))\n",
    "\n",
    "# Compile model\n",
    "lstm_model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=\"adam\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "# Print model summary\n",
    "print(\"\\nModel Summary:\")\n",
    "lstm_model.summary()\n",
    "\n",
    "# Train model\n",
    "history = lstm_model.fit(X_lstm, y_lstm,\n",
    "                        batch_size=64,\n",
    "                        epochs=3,\n",
    "                        validation_split=0.2,\n",
    "                        verbose=1)\n",
    "\n",
    "# Print final metrics\n",
    "print(\"\\nTraining completed!\")\n",
    "print(\"Final training accuracy:\", history.history['accuracy'][-1])\n",
    "print(\"Final validation accuracy:\", history.history['val_accuracy'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-26T19:12:13.067Z"
    },
    "id": "-vAG_GuLhCmf"
   },
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "import swifter\n",
    "from functools import lru_cache\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# First, define and run the ReviewProcessor\n",
    "class ReviewProcessor:\n",
    "    SENTIMENT_MAPPING = {\n",
    "        1: 0,  # Negative\n",
    "        2: 0,  # Negative\n",
    "        3: 1,  # Neutral\n",
    "        4: 2,  # Positive\n",
    "        5: 2   # Positive\n",
    "    }\n",
    "\n",
    "    def __init__(self, reviews_path: str, books_path: str):\n",
    "        self.reviews_path = Path(reviews_path)\n",
    "        self.books_path = Path(books_path)\n",
    "        self.reviews_df = None\n",
    "        self.books_df = None\n",
    "        self.merged_df = None\n",
    "\n",
    "        # Compile regex patterns once\n",
    "        self.punctuation_pattern = re.compile(r'[^a-zA-Z\\s]')\n",
    "        self.spaces_pattern = re.compile(r'\\s+')\n",
    "\n",
    "        # Verify files exist\n",
    "        if not self.reviews_path.exists():\n",
    "            raise FileNotFoundError(f\"Reviews file not found: {reviews_path}\")\n",
    "        if not self.books_path.exists():\n",
    "            raise FileNotFoundError(f\"Books file not found: {books_path}\")\n",
    "\n",
    "    @lru_cache(maxsize=10000)\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "\n",
    "        text = str(text).lower()\n",
    "        text = self.punctuation_pattern.sub('', text)\n",
    "        text = self.spaces_pattern.sub(' ', text)\n",
    "        return text.strip()\n",
    "\n",
    "    def load_data(self) -> None:\n",
    "        self.reviews_df = pd.read_csv(self.reviews_path)\n",
    "        self.books_df = pd.read_csv(self.books_path)\n",
    "\n",
    "        print(f\"Reviews shape: {self.reviews_df.shape}\")\n",
    "        print(f\"Books shape: {self.books_df.shape}\")\n",
    "\n",
    "    def process_reviews(self) -> None:\n",
    "        if 'reviewText' in self.reviews_df.columns:\n",
    "            review_col = 'reviewText'\n",
    "        else:\n",
    "            text_cols = self.reviews_df.select_dtypes(include=['object']).columns\n",
    "            review_col = text_cols[0] if len(text_cols) > 0 else None\n",
    "\n",
    "        if review_col is None:\n",
    "            raise ValueError(\"Could not find review text column\")\n",
    "\n",
    "        self.reviews_df['cleaned_review'] = (\n",
    "            self.reviews_df[review_col]\n",
    "            .apply(self.clean_text)  # Removed swifter for simplicity\n",
    "        )\n",
    "\n",
    "        # Ensure 'overall' column exists before creating 'label'\n",
    "        if 'overall' in self.reviews_df.columns:\n",
    "            self.reviews_df['label'] = self.reviews_df['overall'].map(self.SENTIMENT_MAPPING)\n",
    "        else:\n",
    "            # Handle the case where 'overall' is missing\n",
    "            print(\"Warning: 'overall' column not found in reviews data. 'label' column will not be created.\")\n",
    "            # You might want to raise an error here if 'label' is strictly required later\n",
    "            # raise KeyError(\"'overall' column required to create 'label' column.\")\n",
    "\n",
    "\n",
    "    def merge_data(self) -> pd.DataFrame:\n",
    "        common_cols = set(self.reviews_df.columns) & set(self.books_df.columns)\n",
    "        if not common_cols:\n",
    "            raise ValueError(\"No common columns found for merging datasets\")\n",
    "\n",
    "        merge_col = list(common_cols)[0]\n",
    "        print(f\"Merging on column: {merge_col}\")\n",
    "\n",
    "        self.merged_df = pd.merge(\n",
    "            self.reviews_df,\n",
    "            self.books_df,\n",
    "            on=merge_col,\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "        return self.merged_df\n",
    "\n",
    "    def process(self) -> pd.DataFrame:\n",
    "        self.load_data()\n",
    "        self.process_reviews()\n",
    "        return self.merge_data()\n",
    "\n",
    "# Define the file paths\n",
    "reviews_file_path = \"/home/sagemaker-user/.cache/kagglehub/datasets/mohamedbakhet/amazon-books-reviews/versions/1/Books_rating.csv\"\n",
    "books_details_file_path = \"/home/sagemaker-user/.cache/kagglehub/datasets/mohamedbakhet/amazon-books-reviews/versions/1/books_data.csv\"\n",
    "\n",
    "# Usage\n",
    "processor = ReviewProcessor(reviews_file_path, books_details_file_path)\n",
    "df = processor.process()\n",
    "\n",
    "# Optional: Use a smaller subset of data if needed\n",
    "# df = df.sample(n=10000, random_state=42)\n",
    "\n",
    "print(\"\\nDataFrame shape:\", df.shape)\n",
    "print(\"\\nColumns:\", df.columns.tolist())\n",
    "\n",
    "# Before trying to access df['label'], check if it exists\n",
    "if 'label' in df.columns:\n",
    "    print(\"\\nLabel distribution:\")\n",
    "    print(df['label'].value_counts())\n",
    "else:\n",
    "    print(\"\\n'label' column not found in the processed DataFrame.\")\n",
    "\n",
    "\n",
    "# LSTM model parameters\n",
    "max_words = 5000\n",
    "max_len = 300\n",
    "\n",
    "# Check if 'label' column exists before proceeding with LSTM model\n",
    "if 'label' in df.columns:\n",
    "    # Tokenization\n",
    "    tokenizer_lstm = Tokenizer(num_words=max_words)\n",
    "    tokenizer_lstm.fit_on_texts(df[\"cleaned_review\"])\n",
    "    X_lstm = tokenizer_lstm.texts_to_sequences(df[\"cleaned_review\"])\n",
    "    X_lstm = pad_sequences(X_lstm, maxlen=max_len)\n",
    "    y_lstm = to_categorical(df[\"label\"])\n",
    "\n",
    "    # Create LSTM model\n",
    "    lstm_model = Sequential()\n",
    "    lstm_model.add(Embedding(max_words, 128, input_length=max_len))\n",
    "    lstm_model.add(LSTM(64))\n",
    "    lstm_model.add(Dropout(0.3))\n",
    "    lstm_model.add(Dense(3, activation=\"softmax\"))\n",
    "\n",
    "    # Compile model\n",
    "    lstm_model.compile(loss=\"categorical_crossentropy\",\n",
    "                      optimizer=\"adam\",\n",
    "                      metrics=[\"accuracy\"])\n",
    "\n",
    "    # Print model summary\n",
    "    print(\"\\nModel Summary:\")\n",
    "    lstm_model.summary()\n",
    "\n",
    "    # Train model\n",
    "    history = lstm_model.fit(X_lstm, y_lstm,\n",
    "                            batch_size=64,\n",
    "                            epochs=3,\n",
    "                            validation_split=0.2,\n",
    "                            verbose=1)\n",
    "\n",
    "    # Print final metrics\n",
    "    print(\"\\nTraining completed!\")\n",
    "    print(\"Final training accuracy:\", history.history['accuracy'][-1])\n",
    "    print(\"Final validation accuracy:\", history.history['val_accuracy'][-1])\n",
    "else:\n",
    "    print(\"\\nSkipping LSTM model training as 'label' column is missing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-26T19:13:49.023Z",
     "iopub.execute_input": "2025-06-26T19:13:08.892933Z",
     "iopub.status.busy": "2025-06-26T19:13:08.892277Z"
    },
    "id": "w77twJUGhm05"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 19:13:11.552562: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "# Import all necessary libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "# Ensure swifter is installed and imported if you want to use it later,\n",
    "# but the corrected code removes swifter in the ReviewProcessor.\n",
    "# !pip install swifter\n",
    "# import swifter\n",
    "from functools import lru_cache\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "# Import train_test_split as it's used later in the Bert section\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First, define and run the ReviewProcessor\n",
    "class ReviewProcessor:\n",
    "    SENTIMENT_MAPPING = {\n",
    "        1: 0,  # Negative\n",
    "        2: 0,  # Negative\n",
    "        3: 1,  # Neutral\n",
    "        4: 2,  # Positive\n",
    "        5: 2   # Positive\n",
    "    }\n",
    "\n",
    "    def __init__(self, reviews_path: str, books_path: str):\n",
    "        self.reviews_path = Path(reviews_path)\n",
    "        self.books_path = Path(books_path)\n",
    "        self.reviews_df = None\n",
    "        self.books_df = None\n",
    "        self.merged_df = None\n",
    "\n",
    "        # Compile regex patterns once\n",
    "        self.punctuation_pattern = re.compile(r'[^a-zA-Z\\s]')\n",
    "        self.spaces_pattern = re.compile(r'\\s+')\n",
    "\n",
    "        # Verify files exist\n",
    "        if not self.reviews_path.exists():\n",
    "            raise FileNotFoundError(f\"Reviews file not found: {reviews_path}\")\n",
    "        if not self.books_path.exists():\n",
    "            raise FileNotFoundError(f\"Books file not found: {books_path}\")\n",
    "\n",
    "    # @lru_cache(maxsize=10000) # lru_cache is not effective with pandas Series apply\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "\n",
    "        text = str(text).lower()\n",
    "        text = self.punctuation_pattern.sub('', text)\n",
    "        text = self.spaces_pattern.sub(' ', text)\n",
    "        return text.strip()\n",
    "\n",
    "    def load_data(self) -> None:\n",
    "        print(\"Loading data...\")\n",
    "        self.reviews_df = pd.read_csv(self.reviews_path)\n",
    "        self.books_df = pd.read_csv(self.books_path)\n",
    "\n",
    "        print(f\"Reviews shape: {self.reviews_df.shape}\")\n",
    "        print(f\"Books shape: {self.books_df.shape}\")\n",
    "        print(\"Reviews columns:\", list(self.reviews_df.columns))\n",
    "        print(\"Books columns:\", list(self.books_df.columns))\n",
    "\n",
    "\n",
    "    def process_reviews(self) -> None:\n",
    "        print(\"Processing reviews...\")\n",
    "        review_col = None\n",
    "        # Explicitly check for 'reviewText' and 'review'\n",
    "        possible_review_cols = ['reviewText', 'review']\n",
    "        for col in possible_review_cols:\n",
    "            if col in self.reviews_df.columns:\n",
    "                review_col = col\n",
    "                break\n",
    "\n",
    "        if review_col is None:\n",
    "            # Fallback to finding the first object/text column if known columns aren't found\n",
    "            text_cols = self.reviews_df.select_dtypes(include=['object', 'string']).columns\n",
    "            if len(text_cols) > 0:\n",
    "                 # Prefer columns with 'review' in the name if multiple text columns exist\n",
    "                 review_col = next((c for c in text_cols if 'review' in c.lower()), text_cols[0])\n",
    "\n",
    "\n",
    "        if review_col is None:\n",
    "             # Raise an error if no suitable review column is found\n",
    "            raise ValueError(f\"Could not find a suitable review text column. Looked for {possible_review_cols} or other text columns.\")\n",
    "        else:\n",
    "            print(f\"Using '{review_col}' as the review text column.\")\n",
    "\n",
    "\n",
    "        # Removed swifter as it was causing issues in the original notebook cell\n",
    "        self.reviews_df['cleaned_review'] = self.reviews_df[review_col].apply(self.clean_text)\n",
    "\n",
    "\n",
    "        # --- MODIFIED SECTION ---\n",
    "        # Check for 'review/score' or 'overall' column for sentiment label\n",
    "        score_col = None\n",
    "        if 'review/score' in self.reviews_df.columns:\n",
    "            score_col = 'review/score'\n",
    "        elif 'overall' in self.reviews_df.columns:\n",
    "            score_col = 'overall'\n",
    "\n",
    "        if score_col is not None:\n",
    "            print(f\"Mapping '{score_col}' column to 'label'...\")\n",
    "            # Convert the score column to numeric, coercing errors to NaN\n",
    "            self.reviews_df[score_col] = pd.to_numeric(self.reviews_df[score_col], errors='coerce')\n",
    "            # Drop rows where the score became NaN after coercion\n",
    "            self.reviews_df.dropna(subset=[score_col], inplace=True)\n",
    "            # Map to sentiment labels\n",
    "            self.reviews_df['label'] = self.reviews_df[score_col].map(self.SENTIMENT_MAPPING)\n",
    "            # Drop rows where label mapping resulted in NaN (e.g., if score had values not in mapping)\n",
    "            self.reviews_df.dropna(subset=['label'], inplace=True)\n",
    "            # Convert label to integer type\n",
    "            self.reviews_df['label'] = self.reviews_df['label'].astype(int)\n",
    "        else:\n",
    "            print(\"Warning: Neither 'review/score' nor 'overall' column found in reviews data. Cannot create 'label' column.\")\n",
    "            # Create an empty 'label' column to prevent later KeyErrors if code expects it\n",
    "            self.reviews_df['label'] = pd.NA # Use pandas NA for nullable integer column\n",
    "\n",
    "\n",
    "        # --- END MODIFIED SECTION ---\n",
    "\n",
    "\n",
    "    def merge_data(self) -> pd.DataFrame:\n",
    "        print(\"Merging data...\")\n",
    "        # Identify common columns for merging\n",
    "        common_cols = set(self.reviews_df.columns) & set(self.books_df.columns)\n",
    "        # Exclude columns that are unlikely merge keys like 'cleaned_review', 'label', 'review/score' etc.\n",
    "        # Be careful excluding too many; 'Id' is used in the data and is a valid merge key here.\n",
    "        # Based on column inspection, 'Id' appears to be the merge key.\n",
    "        merge_cols_to_check = [col for col in common_cols if col not in ['cleaned_review', 'label', 'review/helpfulness', 'review/score', 'review/time', 'review/summary', 'review/text', 'description', 'authors', 'image', 'previewLink', 'publisher', 'publishedDate', 'infoLink', 'categories', 'ratingsCount']]\n",
    "\n",
    "\n",
    "        if not merge_cols_to_check:\n",
    "            # Fallback to checking common columns without strict exclusions if primary keys aren't found\n",
    "            merge_cols_to_check = list(common_cols)\n",
    "            if not merge_cols_to_check:\n",
    "                 raise ValueError(\"No suitable common columns found for merging datasets. Check 'Id' or 'asin' in both files.\")\n",
    "            else:\n",
    "                 print(f\"Falling back to checking all common columns for merge key: {merge_cols_to_check}\")\n",
    "\n",
    "\n",
    "        # Prioritize 'Id' or 'asin' if available (based on typical dataset structures and prior run output)\n",
    "        merge_col = None\n",
    "        if 'Id' in merge_cols_to_check:\n",
    "             merge_col = 'Id'\n",
    "        elif 'asin' in merge_cols_to_check:\n",
    "             merge_col = 'asin'\n",
    "        else:\n",
    "            # Use the first common column if preferred ones aren't found\n",
    "            merge_col = list(merge_cols_to_check)[0]\n",
    "\n",
    "\n",
    "        print(f\"Merging on column: {merge_col}\")\n",
    "\n",
    "        # Ensure merge column is of the same type (e.g., string) before merging\n",
    "        # Coerce errors just in case, though 'Id' looks like it should be fine.\n",
    "        self.reviews_df[merge_col] = self.reviews_df[merge_col].astype(str)\n",
    "        self.books_df[merge_col] = self.books_df[merge_col].astype(str)\n",
    "\n",
    "\n",
    "        self.merged_df = pd.merge(\n",
    "            self.reviews_df,\n",
    "            self.books_df,\n",
    "            on=merge_col,\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "        self.optimize_memory()\n",
    "        return self.merged_df\n",
    "\n",
    "    def optimize_memory(self) -> None:\n",
    "        print(\"Optimizing memory...\")\n",
    "        for df_name, df in [('reviews_df', self.reviews_df), ('books_df', self.books_df), ('merged_df', self.merged_df)]:\n",
    "            if df is not None:\n",
    "                initial_memory = df.memory_usage(deep=True).sum() / (1024**2)\n",
    "                # print(f\"Initial memory usage for {df_name}: {initial_memory:.2f} MB\") # Keep this commented unless needed for detailed debugging\n",
    "                for col in df.select_dtypes(include=['object', 'string']).columns:\n",
    "                    # Avoid converting columns with very high cardinality or those intended as text like 'cleaned_review'\n",
    "                    if col in ['cleaned_review', 'review/text', 'review/summary']:\n",
    "                         continue\n",
    "\n",
    "                    if df[col].nunique() / len(df) < 0.5: # Adjust threshold if necessary\n",
    "                        # Check for non-numeric values before converting to category if appropriate\n",
    "                        # Or handle NaNs appropriately depending on downstream use\n",
    "                        if not df[col].isnull().any():\n",
    "                            try:\n",
    "                                # Attempt to convert to category\n",
    "                                df[col] = df[col].astype('category')\n",
    "                            except Exception as e:\n",
    "                                # Handle potential issues during conversion\n",
    "                                print(f\"Could not convert non-null column '{col}' to category: {e}\")\n",
    "                        else:\n",
    "                            # If NaNs are present, convert using nullable category type\n",
    "                             try:\n",
    "                                 df[col] = df[col].astype('category')\n",
    "                             except Exception as e:\n",
    "                                 print(f\"Could not convert nullable column '{col}' to category: {e}\")\n",
    "\n",
    "\n",
    "                final_memory = df.memory_usage(deep=True).sum() / (1024**2)\n",
    "                print(f\"Final memory usage for {df_name}: {final_memory:.2f} MB\")\n",
    "\n",
    "\n",
    "    def process(self) -> pd.DataFrame:\n",
    "        self.load_data()\n",
    "        self.process_reviews()\n",
    "        return self.merge_data()\n",
    "\n",
    "\n",
    "# Define the file paths\n",
    "reviews_file_path = \"/home/sagemaker-user/.cache/kagglehub/datasets/mohamedbakhet/amazon-books-reviews/versions/1/Books_rating.csv\"\n",
    "books_details_file_path = \"/home/sagemaker-user/.cache/kagglehub/datasets/mohamedbakhet/amazon-books-reviews/versions/1/books_data.csv\"\n",
    "\n",
    "# Usage\n",
    "processor = ReviewProcessor(reviews_file_path, books_details_file_path)\n",
    "# You can optionally call inspect_files here to see the columns before processing\n",
    "# processor.inspect_files()\n",
    "result_df = processor.process()\n",
    "\n",
    "# Assign the result to df as the rest of the code expects a dataframe named df\n",
    "df = result_df\n",
    "\n",
    "# Display the first few rows of the result\n",
    "print(\"\\nFirst few rows of processed data:\")\n",
    "# Use display from IPython.display if in a notebook context, otherwise use print\n",
    "try:\n",
    "    from IPython.display import display\n",
    "    display(df.head())\n",
    "except ImportError:\n",
    "    print(df.head())\n",
    "\n",
    "\n",
    "print(\"\\nDataFrame shape:\", df.shape)\n",
    "print(\"\\nColumns:\", df.columns.tolist())\n",
    "\n",
    "# Check if the 'label' column exists before trying to access it\n",
    "if 'label' in df.columns and not df['label'].isnull().all():\n",
    "    print(\"\\nLabel distribution:\")\n",
    "    # Ensure 'label' column has non-null values before value_counts if needed\n",
    "    print(df['label'].value_counts())\n",
    "else:\n",
    "    print(\"Error: 'label' column was not found or contains only null values in the processed DataFrame. Check the presence and format of 'review/score' or 'overall' in the original reviews data.\")\n",
    "\n",
    "# --- Remaining code for LSTM model (assuming 'label' and 'cleaned_review' exist and are suitable) ---\n",
    "# The following code will still fail if 'label' or 'cleaned_review' are not available\n",
    "# or if 'label' contains NaNs after processing.\n",
    "# Add checks here if you want to make this part more robust.\n",
    "\n",
    "if 'cleaned_review' in df.columns and 'label' in df.columns and not df['label'].isnull().all() and len(df['label'].unique()) > 1:\n",
    "    # LSTM model parameters\n",
    "    max_words = 5000\n",
    "    max_len = 300\n",
    "\n",
    "    # Tokenization\n",
    "    print(\"\\nStarting LSTM tokenization...\")\n",
    "    # Drop rows where cleaned_review or label is NaN. Make a copy to avoid SettingWithCopyWarning\n",
    "    df_lstm = df.dropna(subset=['cleaned_review', 'label']).copy()\n",
    "    if df_lstm.empty:\n",
    "        print(\"DataFrame is empty after dropping rows with missing 'cleaned_review' or 'label'. Skipping LSTM.\")\n",
    "    else:\n",
    "        # Ensure label is integer type for to_categorical\n",
    "        df_lstm['label'] = df_lstm['label'].astype(int)\n",
    "\n",
    "        tokenizer_lstm = Tokenizer(num_words=max_words, oov_token=\"<OOV>\") # Added OOV token\n",
    "        tokenizer_lstm.fit_on_texts(df_lstm[\"cleaned_review\"])\n",
    "        X_lstm = tokenizer_lstm.texts_to_sequences(df_lstm[\"cleaned_review\"])\n",
    "        X_lstm = pad_sequences(X_lstm, maxlen=max_len, padding='post', truncating='post') # Added padding/truncating arguments\n",
    "        # Determine number of classes dynamically\n",
    "        num_classes = len(df_lstm['label'].unique())\n",
    "        y_lstm = to_categorical(df_lstm[\"label\"], num_classes=num_classes) # Specify num_classes based on unique labels\n",
    "\n",
    "\n",
    "        # Create LSTM model\n",
    "        print(\"\\nBuilding LSTM model...\")\n",
    "        lstm_model = Sequential()\n",
    "        lstm_model.add(Embedding(max_words, 128, input_length=max_len))\n",
    "        lstm_model.add(LSTM(64))\n",
    "        lstm_model.add(Dropout(0.3))\n",
    "        lstm_model.add(Dense(num_classes, activation=\"softmax\")) # Output layer nodes match number of unique labels\n",
    "\n",
    "        # Compile model\n",
    "        lstm_model.compile(loss=\"categorical_crossentropy\",\n",
    "                          optimizer=\"adam\",\n",
    "                          metrics=[\"accuracy\"])\n",
    "\n",
    "        # Print model summary\n",
    "        print(\"\\nModel Summary:\")\n",
    "        lstm_model.summary()\n",
    "\n",
    "        # Train model\n",
    "        print(\"\\nTraining LSTM model...\")\n",
    "        # Split data before training\n",
    "        X_train_lstm, X_val_lstm, y_train_lstm, y_val_lstm = train_test_split(X_lstm, y_lstm, test_size=0.2, random_state=42, stratify=df_lstm['label']) # Stratify using the label series\n",
    "\n",
    "\n",
    "        history = lstm_model.fit(X_train_lstm, y_train_lstm,\n",
    "                                batch_size=64,\n",
    "                                epochs=3, # Reduced epochs for faster testing if needed\n",
    "                                validation_data=(X_val_lstm, y_val_lstm), # Use dedicated validation data\n",
    "                                verbose=1)\n",
    "\n",
    "        # Print final metrics\n",
    "        print(\"\\nLSTM Training completed!\")\n",
    "        print(\"Final training accuracy:\", history.history['accuracy'][-1])\n",
    "        print(\"Final validation accuracy:\", history.history['val_accuracy'][-1])\n",
    "\n",
    "else:\n",
    "    print(\"\\nSkipping LSTM model training due to missing 'cleaned_review' or 'label' column, all labels are null, or insufficient unique labels (must be > 1).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T19:28:10.741197Z",
     "iopub.status.busy": "2025-06-26T19:28:10.740833Z",
     "iopub.status.idle": "2025-06-26T19:28:12.555041Z",
     "shell.execute_reply": "2025-06-26T19:28:12.554200Z",
     "shell.execute_reply.started": "2025-06-26T19:28:10.741171Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: streamlit in /opt/conda/lib/python3.12/site-packages (1.46.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: kagglehub in /opt/conda/lib/python3.12/site-packages (0.3.12)\n",
      "Requirement already satisfied: plotly in /opt/conda/lib/python3.12/site-packages (6.0.1)\n",
      "Requirement already satisfied: altair<6,>=4.0 in /opt/conda/lib/python3.12/site-packages (from streamlit) (5.5.0)\n",
      "Requirement already satisfied: blinker<2,>=1.5.0 in /opt/conda/lib/python3.12/site-packages (from streamlit) (1.9.0)\n",
      "Requirement already satisfied: cachetools<7,>=4.0 in /opt/conda/lib/python3.12/site-packages (from streamlit) (5.5.2)\n",
      "Requirement already satisfied: click<9,>=7.0 in /opt/conda/lib/python3.12/site-packages (from streamlit) (8.1.8)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in /opt/conda/lib/python3.12/site-packages (from streamlit) (1.26.4)\n",
      "Requirement already satisfied: packaging<26,>=20 in /opt/conda/lib/python3.12/site-packages (from streamlit) (24.2)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in /opt/conda/lib/python3.12/site-packages (from streamlit) (11.2.1)\n",
      "Requirement already satisfied: protobuf<7,>=3.20 in /opt/conda/lib/python3.12/site-packages (from streamlit) (5.28.3)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /opt/conda/lib/python3.12/site-packages (from streamlit) (19.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /opt/conda/lib/python3.12/site-packages (from streamlit) (2.32.3)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in /opt/conda/lib/python3.12/site-packages (from streamlit) (9.1.2)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /opt/conda/lib/python3.12/site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /opt/conda/lib/python3.12/site-packages (from streamlit) (4.13.2)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in /opt/conda/lib/python3.12/site-packages (from streamlit) (6.0.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /opt/conda/lib/python3.12/site-packages (from streamlit) (3.1.44)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /opt/conda/lib/python3.12/site-packages (from streamlit) (0.9.1)\n",
      "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /opt/conda/lib/python3.12/site-packages (from streamlit) (6.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.12/site-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in /opt/conda/lib/python3.12/site-packages (from altair<6,>=4.0->streamlit) (1.38.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.12/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit) (2025.4.26)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.12/site-packages (from kagglehub) (6.0.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from kagglehub) (4.67.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.24.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install streamlit pandas kagglehub plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-26T19:28:43.235Z",
     "iopub.execute_input": "2025-06-26T19:28:13.054227Z",
     "iopub.status.busy": "2025-06-26T19:28:13.053817Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 19:28:14.929 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-26 19:28:14.930 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-26 19:28:15.100 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /opt/conda/lib/python3.12/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-06-26 19:28:15.101 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-26 19:28:15.102 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-26 19:28:15.103 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-26 19:28:15.103 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-26 19:28:15.104 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-26 19:28:15.105 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-26 19:28:15.105 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-26 19:28:15.106 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-26 19:28:15.112 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-26 19:28:15.400 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-26 19:28:15.401 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-26 19:28:15.402 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-26 19:28:15.402 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-26 19:28:15.403 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-26 19:28:15.403 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-26 19:28:15.404 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-26 19:28:15.905 Thread 'Thread-6': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-26 19:28:15.906 Thread 'Thread-6': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-26 19:28:15.907 Thread 'Thread-6': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import kagglehub\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "class ReviewProcessor:\n",
    "    def __init__(self, reviews_path, books_path):\n",
    "        self.reviews_path = Path(reviews_path)\n",
    "        self.books_path = Path(books_path)\n",
    "        self.reviews_df = None\n",
    "        self.books_df = None\n",
    "        self.load_data()\n",
    "    \n",
    "    def load_data(self):\n",
    "        try:\n",
    "            if not (self.reviews_path.exists() and self.books_path.exists()):\n",
    "                with st.spinner('Downloading dataset...'):\n",
    "                    self.download_dataset()\n",
    "            \n",
    "            with st.spinner('Loading data...'):\n",
    "                self.reviews_df = pd.read_csv(self.reviews_path)\n",
    "                self.books_df = pd.read_csv(self.books_path)\n",
    "                st.success('Data loaded successfully!')\n",
    "                \n",
    "        except Exception as e:\n",
    "            st.error(f\"Error loading data: {e}\")\n",
    "            raise\n",
    "\n",
    "    def download_dataset(self):\n",
    "        try:\n",
    "            path = kagglehub.dataset_download(\"mohamedbakhet/amazon-books-reviews\")\n",
    "            self.reviews_path = Path(path) / \"Books_rating.csv\"\n",
    "            self.books_path = Path(path) / \"books_data.csv\"\n",
    "        except Exception as e:\n",
    "            st.error(f\"Error downloading dataset: {e}\")\n",
    "            raise\n",
    "\n",
    "    def get_basic_stats(self):\n",
    "        stats = {\n",
    "            'Total Reviews': len(self.reviews_df),\n",
    "            'Total Books': len(self.books_df),\n",
    "            'Average Rating': round(self.reviews_df['rating'].mean(), 2),\n",
    "            'Median Rating': self.reviews_df['rating'].median()\n",
    "        }\n",
    "        return stats\n",
    "\n",
    "    def create_rating_distribution(self):\n",
    "        rating_counts = self.reviews_df['rating'].value_counts().sort_index()\n",
    "        fig = px.bar(\n",
    "            x=rating_counts.index,\n",
    "            y=rating_counts.values,\n",
    "            title='Distribution of Ratings',\n",
    "            labels={'x': 'Rating', 'y': 'Count'}\n",
    "        )\n",
    "        return fig\n",
    "\n",
    "    def get_top_rated_books(self, min_reviews=100):\n",
    "        # Combine reviews with book details\n",
    "        book_stats = self.reviews_df.groupby('book_id').agg({\n",
    "            'rating': ['mean', 'count']\n",
    "        }).reset_index()\n",
    "        \n",
    "        book_stats.columns = ['book_id', 'avg_rating', 'review_count']\n",
    "        \n",
    "        # Filter books with minimum number of reviews\n",
    "        qualified_books = book_stats[book_stats['review_count'] >= min_reviews]\n",
    "        \n",
    "        # Merge with book details\n",
    "        top_books = qualified_books.merge(self.books_df, on='book_id')\n",
    "        \n",
    "        # Sort by average rating\n",
    "        return top_books.sort_values('avg_rating', ascending=False).head(10)\n",
    "\n",
    "def main():\n",
    "    st.set_page_config(\n",
    "        page_title=\"Amazon Book Reviews Analysis\",\n",
    "        page_icon=\"\",\n",
    "        layout=\"wide\"\n",
    "    )\n",
    "\n",
    "    st.title(\" Amazon Book Reviews Analysis\")\n",
    "    st.markdown(\"---\")\n",
    "\n",
    "    # Initialize ReviewProcessor\n",
    "    try:\n",
    "        processor = ReviewProcessor(\n",
    "            reviews_path=\"Books_rating.csv\",\n",
    "            books_path=\"books_data.csv\"\n",
    "        )\n",
    "\n",
    "        # Create sidebar\n",
    "        st.sidebar.header(\"Analysis Options\")\n",
    "        min_reviews = st.sidebar.slider(\n",
    "            \"Minimum number of reviews for top books\",\n",
    "            min_value=10,\n",
    "            max_value=500,\n",
    "            value=100,\n",
    "            step=10\n",
    "        )\n",
    "\n",
    "        # Create main layout with columns\n",
    "        col1, col2 = st.columns(2)\n",
    "\n",
    "        # Display basic stats in the first column\n",
    "        with col1:\n",
    "            st.subheader(\" Basic Statistics\")\n",
    "            stats = processor.get_basic_stats()\n",
    "            for stat_name, stat_value in stats.items():\n",
    "                st.metric(label=stat_name, value=stat_value)\n",
    "\n",
    "        # Display rating distribution in the second column\n",
    "        with col2:\n",
    "            st.subheader(\" Rating Distribution\")\n",
    "            rating_dist = processor.create_rating_distribution()\n",
    "            st.plotly_chart(rating_dist, use_container_width=True)\n",
    "\n",
    "        # Display top rated books\n",
    "        st.subheader(\" Top Rated Books\")\n",
    "        top_books = processor.get_top_rated_books(min_reviews=min_reviews)\n",
    "        \n",
    "        # Create a formatted table for top books\n",
    "        for idx, book in top_books.iterrows():\n",
    "            with st.container():\n",
    "                col1, col2 = st.columns([1, 3])\n",
    "                with col1:\n",
    "                    st.image(book['image_url'] if 'image_url' in book else \"https://placeholder.com/150\", \n",
    "                            width=100)\n",
    "                with col2:\n",
    "                    st.markdown(f\"**{book['title']}**\")\n",
    "                    st.write(f\"Author: {book['author']}\")\n",
    "                    st.write(f\"Average Rating:  {book['avg_rating']:.2f} ({book['review_count']} reviews)\")\n",
    "                st.markdown(\"---\")\n",
    "\n",
    "    except Exception as e:\n",
    "        st.error(f\"An error occurred: {e}\")\n",
    "        st.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T19:35:48.430972Z",
     "iopub.status.busy": "2025-06-26T19:35:48.430686Z",
     "iopub.status.idle": "2025-06-26T19:35:48.698821Z",
     "shell.execute_reply": "2025-06-26T19:35:48.697719Z",
     "shell.execute_reply.started": "2025-06-26T19:35:48.430950Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for data files...\n",
      "Downloading dataset...\n",
      "Loading data...\n",
      "\n",
      "Reviews DataFrame columns: ['Id', 'Title', 'Price', 'User_id', 'profileName', 'review/helpfulness', 'review/score', 'review/time', 'review/summary', 'review/text']\n",
      "Books DataFrame columns: ['Title', 'description', 'authors', 'image', 'previewLink', 'publisher', 'publishedDate', 'infoLink', 'categories', 'ratingsCount']\n",
      "\n",
      "Data loaded successfully!\n",
      "Reviews shape: (5, 10)\n",
      "Books shape: (5, 10)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from pathlib import Path\n",
    "import kagglehub\n",
    "from IPython.display import display, HTML\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "class ReviewProcessor:\n",
    "    def __init__(self):\n",
    "        self.reviews_df = None\n",
    "        self.books_df = None\n",
    "        self.data_dir = Path(\"data\")\n",
    "        self.reviews_path = self.data_dir / \"Books_rating.csv\"\n",
    "        self.books_path = self.data_dir / \"books_data.csv\"\n",
    "        \n",
    "    def load_data(self, sample_size=None):\n",
    "        try:\n",
    "            print(\"Checking for data files...\")\n",
    "            \n",
    "            if not (self.reviews_path.exists() and self.books_path.exists()):\n",
    "                print(\"Downloading dataset...\")\n",
    "                path = kagglehub.dataset_download(\"mohamedbakhet/amazon-books-reviews\")\n",
    "                self.reviews_path = Path(path) / \"Books_rating.csv\"\n",
    "                self.books_path = Path(path) / \"books_data.csv\"\n",
    "            \n",
    "            print(\"Loading data...\")\n",
    "            if sample_size:\n",
    "                self.reviews_df = pd.read_csv(self.reviews_path, nrows=sample_size)\n",
    "                self.books_df = pd.read_csv(self.books_path, nrows=sample_size)\n",
    "            else:\n",
    "                chunks = []\n",
    "                for chunk in tqdm(pd.read_csv(self.reviews_path, chunksize=100000), desc=\"Loading reviews\"):\n",
    "                    chunks.append(chunk)\n",
    "                self.reviews_df = pd.concat(chunks)\n",
    "                self.books_df = pd.read_csv(self.books_path)\n",
    "            \n",
    "            # Print column names to debug\n",
    "            print(\"\\nReviews DataFrame columns:\", self.reviews_df.columns.tolist())\n",
    "            print(\"Books DataFrame columns:\", self.books_df.columns.tolist())\n",
    "            \n",
    "            print(f\"\\nData loaded successfully!\")\n",
    "            print(f\"Reviews shape: {self.reviews_df.shape}\")\n",
    "            print(f\"Books shape: {self.books_df.shape}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            \n",
    "    def get_basic_stats(self):\n",
    "        if self.reviews_df is None or self.books_df is None:\n",
    "            return\n",
    "            \n",
    "        print(\"Calculating basic statistics...\")\n",
    "        \n",
    "        # First check if 'Rating' or 'rating' exists\n",
    "        rating_column = None\n",
    "        if 'Rating' in self.reviews_df.columns:\n",
    "            rating_column = 'Rating'\n",
    "        elif 'rating' in self.reviews_df.columns:\n",
    "            rating_column = 'rating'\n",
    "        else:\n",
    "            print(\"Warning: Rating column not found. Available columns:\", self.reviews_df.columns.tolist())\n",
    "            return\n",
    "            \n",
    "        stats = {\n",
    "            'Total Reviews': len(self.reviews_df),\n",
    "            'Total Books': len(self.books_df),\n",
    "            'Average Rating': round(self.reviews_df[rating_column].mean(), 2),\n",
    "            'Median Rating': self.reviews_df[rating_column].median()\n",
    "        }\n",
    "        \n",
    "        stats_df = pd.DataFrame(list(stats.items()), columns=['Metric', 'Value'])\n",
    "        display(HTML(\"<h3>Basic Statistics</h3>\"))\n",
    "        display(stats_df)\n",
    "        \n",
    "    def plot_rating_distribution(self):\n",
    "        if self.reviews_df is None:\n",
    "            return\n",
    "            \n",
    "        # Check for rating column\n",
    "        rating_column = None\n",
    "        if 'Rating' in self.reviews_df.columns:\n",
    "            rating_column = 'Rating'\n",
    "        elif 'rating' in self.reviews_df.columns:\n",
    "            rating_column = 'rating'\n",
    "        else:\n",
    "            print(\"Warning: Rating column not found\")\n",
    "            return\n",
    "            \n",
    "        print(\"Creating rating distribution plot...\")\n",
    "        rating_counts = self.reviews_df[rating_column].value_counts().sort_index()\n",
    "        fig = px.bar(\n",
    "            x=rating_counts.index,\n",
    "            y=rating_counts.values,\n",
    "            title='Distribution of Ratings',\n",
    "            labels={'x': 'Rating', 'y': 'Count'}\n",
    "        )\n",
    "        fig.show()\n",
    "        \n",
    "    def get_top_rated_books(self, min_reviews=100):\n",
    "        if self.reviews_df is None or self.books_df is None:\n",
    "            return\n",
    "            \n",
    "        # Check for rating column\n",
    "        rating_column = None\n",
    "        if 'Rating' in self.reviews_df.columns:\n",
    "            rating_column = 'Rating'\n",
    "        elif 'rating' in self.reviews_df.columns:\n",
    "            rating_column = 'rating'\n",
    "        else:\n",
    "            print(\"Warning: Rating column not found\")\n",
    "            return\n",
    "            \n",
    "        print(\"Analyzing top rated books...\")\n",
    "        book_stats = self.reviews_df.groupby('book_id').agg({\n",
    "            rating_column: ['mean', 'count']\n",
    "        }).reset_index()\n",
    "        \n",
    "        book_stats.columns = ['book_id', 'avg_rating', 'review_count']\n",
    "        qualified_books = book_stats[book_stats['review_count'] >= min_reviews]\n",
    "        top_books = qualified_books.merge(self.books_df, on='book_id')\n",
    "        top_books = top_books.sort_values('avg_rating', ascending=False).head(10)\n",
    "        \n",
    "        display(HTML(\"<h3>Top Rated Books</h3>\"))\n",
    "        display(HTML(f\"<p>Showing books with at least {min_reviews} reviews</p>\"))\n",
    "        \n",
    "        display_cols = ['title', 'author', 'avg_rating', 'review_count']\n",
    "        formatted_books = top_books[display_cols].copy()\n",
    "        formatted_books['avg_rating'] = formatted_books['avg_rating'].round(2)\n",
    "        formatted_books.columns = ['Title', 'Author', 'Average Rating', 'Number of Reviews']\n",
    "        display(formatted_books)\n",
    "\n",
    "def analyze_amazon_books(sample_size=None):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"Starting analysis...\")\n",
    "    processor = ReviewProcessor()\n",
    "    \n",
    "    # Load data with optional sampling\n",
    "    processor.load_data(sample_size=sample_size)\n",
    "    \n",
    "    # Get basic statistics\n",
    "    processor.get_basic_stats()\n",
    "    \n",
    "    # Plot rating distribution\n",
    "    processor.plot_rating_distribution()\n",
    "    \n",
    "    # Get top rated books\n",
    "    processor.get_top_rated_books(min_reviews=100)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"\\nTotal analysis time: {round(end_time - start_time, 2)} seconds\")\n",
    "\n",
    "# First, let's examine the data structure\n",
    "processor = ReviewProcessor()\n",
    "processor.load_data(sample_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T19:37:01.330618Z",
     "iopub.status.busy": "2025-06-26T19:37:01.329710Z",
     "iopub.status.idle": "2025-06-26T19:37:03.020116Z",
     "shell.execute_reply": "2025-06-26T19:37:03.019248Z",
     "shell.execute_reply.started": "2025-06-26T19:37:01.330583Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting analysis...\n",
      "Checking for data files...\n",
      "Downloading dataset...\n",
      "Loading data...\n",
      "\n",
      "Reviews DataFrame columns: ['Id', 'Title', 'Price', 'User_id', 'profileName', 'review/helpfulness', 'review/score', 'review/time', 'review/summary', 'review/text']\n",
      "Books DataFrame columns: ['Title', 'description', 'authors', 'image', 'previewLink', 'publisher', 'publishedDate', 'infoLink', 'categories', 'ratingsCount']\n",
      "\n",
      "Data loaded successfully!\n",
      "Reviews shape: (10000, 10)\n",
      "Books shape: (10000, 10)\n",
      "Calculating basic statistics...\n",
      "Warning: Rating column not found. Available columns: ['Id', 'Title', 'Price', 'User_id', 'profileName', 'review/helpfulness', 'review/score', 'review/time', 'review/summary', 'review/text']\n",
      "Warning: Rating column not found\n",
      "Warning: Rating column not found\n",
      "\n",
      "Total analysis time: 1.65 seconds\n"
     ]
    }
   ],
   "source": [
    "# Run with larger sample after confirming column names\n",
    "analyze_amazon_books(sample_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T23:56:45.410988Z",
     "iopub.status.busy": "2025-06-26T23:56:45.410714Z",
     "iopub.status.idle": "2025-06-26T23:56:45.443746Z",
     "shell.execute_reply": "2025-06-26T23:56:45.442957Z",
     "shell.execute_reply.started": "2025-06-26T23:56:45.410967Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Book Review Analyzer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29464e8c566a48d2a7bec8ff45b34d60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Button(button_style='primary', description='Load Data', icon='database', style=B"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import ipywidgets as widgets\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "class ReviewAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.reviews_df = None\n",
    "        self.books_df = None\n",
    "        # Direct links to the dataset files\n",
    "        self.reviews_url = \"https://raw.githubusercontent.com/zygmuntz/goodbooks-10k/master/ratings.csv\"\n",
    "        self.books_url = \"https://raw.githubusercontent.com/zygmuntz/goodbooks-10k/master/books.csv\"\n",
    "        self.setup_ui()\n",
    "        \n",
    "    def download_data(self):\n",
    "        \"\"\"Download the dataset files\"\"\"\n",
    "        try:\n",
    "            print(\"Downloading reviews data...\")\n",
    "            self.reviews_df = pd.read_csv(self.reviews_url, nrows=self.sample_size.value)\n",
    "            \n",
    "            print(\"Downloading books data...\")\n",
    "            self.books_df = pd.read_csv(self.books_url)\n",
    "            \n",
    "            print(\"Data downloaded successfully!\")\n",
    "            print(f\"Reviews shape: {self.reviews_df.shape}\")\n",
    "            print(f\"Books shape: {self.books_df.shape}\")\n",
    "            \n",
    "            # Display sample of the data\n",
    "            print(\"\\nSample of reviews data:\")\n",
    "            display(self.reviews_df.head())\n",
    "            print(\"\\nSample of books data:\")\n",
    "            display(self.books_df.head())\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading data: {e}\")\n",
    "            return False\n",
    "\n",
    "    def setup_ui(self):\n",
    "        # Create UI components\n",
    "        self.load_button = widgets.Button(\n",
    "            description='Load Data',\n",
    "            button_style='primary',\n",
    "            icon='database'\n",
    "        )\n",
    "        \n",
    "        self.sample_size = widgets.IntText(\n",
    "            value=10000,\n",
    "            description='Sample Size:',\n",
    "            disabled=False\n",
    "        )\n",
    "        \n",
    "        self.min_reviews = widgets.IntSlider(\n",
    "            value=100,\n",
    "            min=10,\n",
    "            max=1000,\n",
    "            step=10,\n",
    "            description='Min Reviews:',\n",
    "            disabled=False\n",
    "        )\n",
    "        \n",
    "        # Create tabs\n",
    "        self.stats_output = widgets.Output()\n",
    "        self.dist_output = widgets.Output()\n",
    "        self.books_output = widgets.Output()\n",
    "        \n",
    "        self.tabs = widgets.Tab(children=[\n",
    "            self.stats_output,\n",
    "            self.dist_output,\n",
    "            self.books_output\n",
    "        ])\n",
    "        \n",
    "        self.tabs.set_title(0, 'Basic Stats')\n",
    "        self.tabs.set_title(1, 'Rating Distribution')\n",
    "        self.tabs.set_title(2, 'Top Books')\n",
    "        \n",
    "        # Set up callbacks\n",
    "        self.load_button.on_click(self.on_load_click)\n",
    "        \n",
    "        # Display UI\n",
    "        display(widgets.VBox([\n",
    "            widgets.HBox([self.load_button, self.sample_size]),\n",
    "            self.min_reviews,\n",
    "            self.tabs\n",
    "        ]))\n",
    "\n",
    "    def on_load_click(self, b):\n",
    "        with self.stats_output:\n",
    "            clear_output()\n",
    "            if self.download_data():\n",
    "                self.update_all_tabs()\n",
    "\n",
    "    def update_all_tabs(self):\n",
    "        self.update_stats_tab()\n",
    "        self.update_distribution_tab()\n",
    "        self.update_top_books_tab()\n",
    "\n",
    "    def update_stats_tab(self):\n",
    "        with self.stats_output:\n",
    "            clear_output()\n",
    "            if self.reviews_df is None:\n",
    "                print(\"Please load data first\")\n",
    "                return\n",
    "                \n",
    "            stats = {\n",
    "                'Total Reviews': len(self.reviews_df),\n",
    "                'Total Books': len(self.books_df),\n",
    "                'Average Rating': round(self.reviews_df['rating'].mean(), 2),\n",
    "                'Median Rating': self.reviews_df['rating'].median()\n",
    "            }\n",
    "            \n",
    "            stats_df = pd.DataFrame(list(stats.items()), columns=['Metric', 'Value'])\n",
    "            display(HTML(\"<h3>Basic Statistics</h3>\"))\n",
    "            display(stats_df)\n",
    "\n",
    "    def update_distribution_tab(self):\n",
    "        with self.dist_output:\n",
    "            clear_output()\n",
    "            if self.reviews_df is None:\n",
    "                print(\"Please load data first\")\n",
    "                return\n",
    "                \n",
    "            rating_counts = self.reviews_df['rating'].value_counts().sort_index()\n",
    "            fig = px.bar(\n",
    "                x=rating_counts.index,\n",
    "                y=rating_counts.values,\n",
    "                title='Distribution of Ratings',\n",
    "                labels={'x': 'Rating', 'y': 'Count'}\n",
    "            )\n",
    "            fig.show()\n",
    "\n",
    "    def update_top_books_tab(self):\n",
    "        with self.books_output:\n",
    "            clear_output()\n",
    "            if self.reviews_df is None:\n",
    "                print(\"Please load data first\")\n",
    "                return\n",
    "                \n",
    "            # Calculate book statistics\n",
    "            book_stats = self.reviews_df.groupby('book_id').agg({\n",
    "                'rating': ['mean', 'count']\n",
    "            }).reset_index()\n",
    "            \n",
    "            book_stats.columns = ['book_id', 'avg_rating', 'review_count']\n",
    "            qualified_books = book_stats[book_stats['review_count'] >= self.min_reviews.value]\n",
    "            top_books = qualified_books.merge(self.books_df, on='book_id')\n",
    "            top_books = top_books.sort_values('avg_rating', ascending=False).head(10)\n",
    "            \n",
    "            # Format the display\n",
    "            display_cols = ['title', 'authors', 'avg_rating', 'review_count']\n",
    "            formatted_books = top_books[display_cols].copy()\n",
    "            formatted_books['avg_rating'] = formatted_books['avg_rating'].round(2)\n",
    "            formatted_books.columns = ['Title', 'Author', 'Average Rating', 'Number of Reviews']\n",
    "            \n",
    "            display(HTML(\"<h3>Top Rated Books</h3>\"))\n",
    "            display(HTML(f\"<p>Showing books with at least {self.min_reviews.value} reviews</p>\"))\n",
    "            display(formatted_books)\n",
    "\n",
    "# First make sure we have all required packages\n",
    "try:\n",
    "    import ipywidgets\n",
    "except ImportError:\n",
    "    print(\"Installing required packages...\")\n",
    "    !pip install ipywidgets\n",
    "\n",
    "# Create and display the analyzer\n",
    "print(\"Loading Book Review Analyzer...\")\n",
    "analyzer = ReviewAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 2476732,
     "sourceId": 4200454,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4296815,
     "sourceId": 7391354,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4304376,
     "sourceId": 7402225,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4338523,
     "sourceId": 7453808,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4350935,
     "sourceId": 7473574,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4361279,
     "sourceId": 7490778,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
