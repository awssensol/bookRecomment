{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 4200454,
          "sourceType": "datasetVersion",
          "datasetId": 2476732
        },
        {
          "sourceId": 7391354,
          "sourceType": "datasetVersion",
          "datasetId": 4296815
        },
        {
          "sourceId": 7402225,
          "sourceType": "datasetVersion",
          "datasetId": 4304376
        },
        {
          "sourceId": 7453808,
          "sourceType": "datasetVersion",
          "datasetId": 4338523
        },
        {
          "sourceId": 7473574,
          "sourceType": "datasetVersion",
          "datasetId": 4350935
        },
        {
          "sourceId": 7490778,
          "sourceType": "datasetVersion",
          "datasetId": 4361279
        }
      ],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/awssensol/bookRecomment/blob/main/Copy_of_Book_Recommendation_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"mohamedbakhet/amazon-books-reviews\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mi_w1ImXwlS",
        "outputId": "851a9cf8-ec43-4587-bf0d-9a90ca8d9426"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/amazon-books-reviews\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas boto3 transformers torch keras scikit-learn nltk\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "import boto3\n",
        "import nltk\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoHTvx3cX2gM",
        "outputId": "317a2078-41cb-495c-ffda-044f461e9e1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.38.35-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (3.8.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Collecting botocore<1.39.0,>=1.38.35 (from boto3)\n",
            "  Downloading botocore-1.38.35-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.14.0,>=0.13.0 (from boto3)\n",
            "  Downloading s3transfer-0.13.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.32.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras) (3.13.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras) (0.16.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras) (0.4.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore<1.39.0,>=1.38.35->boto3) (2.4.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Downloading boto3-1.38.35-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m93.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.38.35-py3-none-any.whl (13.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m98.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.13.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.2/85.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, jmespath, nvidia-cusparse-cu12, nvidia-cudnn-cu12, botocore, s3transfer, nvidia-cusolver-cu12, boto3\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed boto3-1.38.35 botocore-1.38.35 jmespath-1.0.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 s3transfer-0.13.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install swifter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxvLr3hDcINS",
        "outputId": "e7f43cb7-64ef-4305-9dc1-0e18ff6bd968"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting swifter\n",
            "  Downloading swifter-1.4.0.tar.gz (1.2 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.2 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from swifter) (2.2.2)\n",
            "Requirement already satisfied: psutil>=5.6.6 in /usr/local/lib/python3.11/dist-packages (from swifter) (5.9.5)\n",
            "Requirement already satisfied: dask>=2.10.0 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]>=2.10.0->swifter) (2024.12.1)\n",
            "Requirement already satisfied: tqdm>=4.33.0 in /usr/local/lib/python3.11/dist-packages (from swifter) (4.67.1)\n",
            "Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.11/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (8.2.1)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (3.1.1)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.11/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (24.2)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (1.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (6.0.2)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (0.12.1)\n",
            "Requirement already satisfied: importlib_metadata>=4.13.0 in /usr/local/lib/python3.11/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (8.7.0)\n",
            "Requirement already satisfied: dask-expr<1.2,>=1.1 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]>=2.10.0->swifter) (1.1.21)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->swifter) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->swifter) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->swifter) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->swifter) (2025.2)\n",
            "Requirement already satisfied: pyarrow>=14.0.1 in /usr/local/lib/python3.11/dist-packages (from dask-expr<1.2,>=1.1->dask[dataframe]>=2.10.0->swifter) (18.1.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata>=4.13.0->dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (3.22.0)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.11/dist-packages (from partd>=1.4.0->dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->swifter) (1.17.0)\n",
            "Building wheels for collected packages: swifter\n",
            "  Building wheel for swifter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for swifter: filename=swifter-1.4.0-py3-none-any.whl size=16505 sha256=24f58677162e131be7c408324e015c539ccccd41e818ce205ffc1c0c11495cfd\n",
            "  Stored in directory: /root/.cache/pip/wheels/ef/7f/bd/9bed48f078f3ee1fa75e0b29b6e0335ce1cb03a38d3443b3a3\n",
            "Successfully built swifter\n",
            "Installing collected packages: swifter\n",
            "Successfully installed swifter-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any\n",
        "import swifter\n",
        "from functools import lru_cache\n",
        "\n",
        "class ReviewProcessor:\n",
        "    SENTIMENT_MAPPING = {\n",
        "        1: 0,  # Negative\n",
        "        2: 0,  # Negative\n",
        "        3: 1,  # Neutral\n",
        "        4: 2,  # Positive\n",
        "        5: 2   # Positive\n",
        "    }\n",
        "\n",
        "    def __init__(self, reviews_path: str, books_path: str):\n",
        "        self.reviews_path = Path(reviews_path)\n",
        "        self.books_path = Path(books_path)\n",
        "        self.reviews_df = None\n",
        "        self.books_df = None\n",
        "        self.merged_df = None\n",
        "\n",
        "        # Compile regex patterns once\n",
        "        self.punctuation_pattern = re.compile(r'[^a-zA-Z\\s]')\n",
        "        self.spaces_pattern = re.compile(r'\\s+')\n",
        "\n",
        "        # Verify files exist\n",
        "        if not self.reviews_path.exists():\n",
        "            raise FileNotFoundError(f\"Reviews file not found: {reviews_path}\")\n",
        "        if not self.books_path.exists():\n",
        "            raise FileNotFoundError(f\"Books file not found: {books_path}\")\n",
        "\n",
        "    def inspect_files(self):\n",
        "        \"\"\"\n",
        "        Print the column names of both CSV files\n",
        "        \"\"\"\n",
        "        # Read just the headers\n",
        "        reviews_cols = pd.read_csv(self.reviews_path, nrows=0).columns\n",
        "        books_cols = pd.read_csv(self.books_path, nrows=0).columns\n",
        "\n",
        "        print(\"Reviews columns:\", list(reviews_cols))\n",
        "        print(\"Books columns:\", list(books_cols))\n",
        "        return reviews_cols, books_cols\n",
        "\n",
        "    @lru_cache(maxsize=10000)\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        if pd.isna(text):\n",
        "            return \"\"\n",
        "\n",
        "        text = str(text).lower()\n",
        "        text = self.punctuation_pattern.sub('', text)\n",
        "        text = self.spaces_pattern.sub(' ', text)\n",
        "        return text.strip()\n",
        "\n",
        "    def load_data(self) -> None:\n",
        "        # First inspect the columns\n",
        "        reviews_cols, books_cols = self.inspect_files()\n",
        "\n",
        "        dtypes = {\n",
        "            'reviewerID': 'category',\n",
        "            'asin': 'category',\n",
        "            'overall': 'float32'\n",
        "        }\n",
        "\n",
        "        # Read the files with the actual column names\n",
        "        self.reviews_df = pd.read_csv(self.reviews_path)\n",
        "        self.books_df = pd.read_csv(self.books_path)\n",
        "\n",
        "        # Print shape information\n",
        "        print(f\"Reviews shape: {self.reviews_df.shape}\")\n",
        "        print(f\"Books shape: {self.books_df.shape}\")\n",
        "\n",
        "    def process_reviews(self) -> None:\n",
        "        if 'reviewText' in self.reviews_df.columns:\n",
        "            review_col = 'reviewText'\n",
        "        else:\n",
        "            # Try to find a similar column name or use the first text column\n",
        "            text_cols = self.reviews_df.select_dtypes(include=['object']).columns\n",
        "            review_col = text_cols[0] if len(text_cols) > 0 else None\n",
        "\n",
        "        if review_col is None:\n",
        "            raise ValueError(\"Could not find review text column\")\n",
        "\n",
        "        self.reviews_df['cleaned_review'] = (\n",
        "            self.reviews_df[review_col]\n",
        "            .swifter.apply(self.clean_text)\n",
        "        )\n",
        "\n",
        "        if 'overall' in self.reviews_df.columns:\n",
        "            self.reviews_df['label'] = self.reviews_df['overall'].map(self.SENTIMENT_MAPPING)\n",
        "\n",
        "    def merge_data(self) -> pd.DataFrame:\n",
        "        # Identify common columns for merging\n",
        "        common_cols = set(self.reviews_df.columns) & set(self.books_df.columns)\n",
        "        if not common_cols:\n",
        "            raise ValueError(\"No common columns found for merging datasets\")\n",
        "\n",
        "        merge_col = list(common_cols)[0]  # Use the first common column\n",
        "        print(f\"Merging on column: {merge_col}\")\n",
        "\n",
        "        self.merged_df = pd.merge(\n",
        "            self.reviews_df,\n",
        "            self.books_df,\n",
        "            on=merge_col,\n",
        "            how='left'\n",
        "        )\n",
        "\n",
        "        self.optimize_memory()\n",
        "        return self.merged_df\n",
        "\n",
        "    def optimize_memory(self) -> None:\n",
        "        for df in [self.reviews_df, self.books_df, self.merged_df]:\n",
        "            if df is not None:\n",
        "                for col in df.select_dtypes(include=['object']).columns:\n",
        "                    if df[col].nunique() / len(df) < 0.5:\n",
        "                        df[col] = df[col].astype('category')\n",
        "\n",
        "    def process(self) -> pd.DataFrame:\n",
        "        self.load_data()\n",
        "        self.process_reviews()\n",
        "        return self.merge_data()\n",
        "\n",
        "\n",
        "# Define the file paths\n",
        "reviews_file_path = \"/kaggle/input/amazon-books-reviews/Books_rating.csv\"\n",
        "books_details_file_path = \"/kaggle/input/amazon-books-reviews/books_data.csv\"\n",
        "\n",
        "# Usage\n",
        "processor = ReviewProcessor(reviews_file_path, books_details_file_path)\n",
        "# First inspect the files\n",
        "processor.inspect_files()\n",
        "# Then process\n",
        "result_df = processor.process()\n",
        "\n",
        "# Display the first few rows of the result\n",
        "print(\"\\nFirst few rows of processed data:\")\n",
        "display(result_df.head())"
      ],
      "metadata": {
        "id": "yh8FexfEbgq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "X_train, X_val, y_train, y_val = train_test_split(df[\"cleaned_review\"], df[\"label\"], test_size=0.2)\n",
        "\n",
        "train_encodings = tokenizer(list(X_train), truncation=True, padding=True)\n",
        "val_encodings = tokenizer(list(X_val), truncation=True, padding=True)\n",
        "\n",
        "class ReviewDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "    def __getitem__(self, idx):\n",
        "        return {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}, torch.tensor(self.labels[idx])\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = ReviewDataset(train_encodings, y_train.tolist())\n",
        "val_dataset = ReviewDataset(val_encodings, y_val.tolist())\n",
        "\n",
        "bert_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./bert_results\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=8,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_total_limit=1,\n",
        "    logging_dir=\"./bert_logs\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(model=bert_model, args=training_args, train_dataset=train_dataset, eval_dataset=val_dataset)\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "1HW8uaHkciL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import all necessary libraries\n",
        "import pandas as pd\n",
        "import re\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any\n",
        "import swifter\n",
        "from functools import lru_cache\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "\n",
        "# First, define and run the ReviewProcessor\n",
        "class ReviewProcessor:\n",
        "    SENTIMENT_MAPPING = {\n",
        "        1: 0,  # Negative\n",
        "        2: 0,  # Negative\n",
        "        3: 1,  # Neutral\n",
        "        4: 2,  # Positive\n",
        "        5: 2   # Positive\n",
        "    }\n",
        "\n",
        "    def __init__(self, reviews_path: str, books_path: str):\n",
        "        self.reviews_path = Path(reviews_path)\n",
        "        self.books_path = Path(books_path)\n",
        "        self.reviews_df = None\n",
        "        self.books_df = None\n",
        "        self.merged_df = None\n",
        "\n",
        "        # Compile regex patterns once\n",
        "        self.punctuation_pattern = re.compile(r'[^a-zA-Z\\s]')\n",
        "        self.spaces_pattern = re.compile(r'\\s+')\n",
        "\n",
        "        # Verify files exist\n",
        "        if not self.reviews_path.exists():\n",
        "            raise FileNotFoundError(f\"Reviews file not found: {reviews_path}\")\n",
        "        if not self.books_path.exists():\n",
        "            raise FileNotFoundError(f\"Books file not found: {books_path}\")\n",
        "\n",
        "    @lru_cache(maxsize=10000)\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        if pd.isna(text):\n",
        "            return \"\"\n",
        "\n",
        "        text = str(text).lower()\n",
        "        text = self.punctuation_pattern.sub('', text)\n",
        "        text = self.spaces_pattern.sub(' ', text)\n",
        "        return text.strip()\n",
        "\n",
        "    def load_data(self) -> None:\n",
        "        self.reviews_df = pd.read_csv(self.reviews_path)\n",
        "        self.books_df = pd.read_csv(self.books_path)\n",
        "\n",
        "        print(f\"Reviews shape: {self.reviews_df.shape}\")\n",
        "        print(f\"Books shape: {self.books_df.shape}\")\n",
        "\n",
        "    def process_reviews(self) -> None:\n",
        "        if 'reviewText' in self.reviews_df.columns:\n",
        "            review_col = 'reviewText'\n",
        "        else:\n",
        "            text_cols = self.reviews_df.select_dtypes(include=['object']).columns\n",
        "            review_col = text_cols[0] if len(text_cols) > 0 else None\n",
        "\n",
        "        if review_col is None:\n",
        "            raise ValueError(\"Could not find review text column\")\n",
        "\n",
        "        self.reviews_df['cleaned_review'] = (\n",
        "            self.reviews_df[review_col]\n",
        "            .apply(self.clean_text)  # Removed swifter for simplicity\n",
        "        )\n",
        "\n",
        "        if 'overall' in self.reviews_df.columns:\n",
        "            self.reviews_df['label'] = self.reviews_df['overall'].map(self.SENTIMENT_MAPPING)\n",
        "\n",
        "    def merge_data(self) -> pd.DataFrame:\n",
        "        common_cols = set(self.reviews_df.columns) & set(self.books_df.columns)\n",
        "        if not common_cols:\n",
        "            raise ValueError(\"No common columns found for merging datasets\")\n",
        "\n",
        "        merge_col = list(common_cols)[0]\n",
        "        print(f\"Merging on column: {merge_col}\")\n",
        "\n",
        "        self.merged_df = pd.merge(\n",
        "            self.reviews_df,\n",
        "            self.books_df,\n",
        "            on=merge_col,\n",
        "            how='left'\n",
        "        )\n",
        "\n",
        "        return self.merged_df\n",
        "\n",
        "    def process(self) -> pd.DataFrame:\n",
        "        self.load_data()\n",
        "        self.process_reviews()\n",
        "        return self.merge_data()\n",
        "\n",
        "# Process the data\n",
        "reviews_file_path = \"/kaggle/input/amazon-books-reviews/Books_rating.csv\"\n",
        "books_details_file_path = \"/kaggle/input/amazon-books-reviews/books_data.csv\"\n",
        "\n",
        "processor = ReviewProcessor(reviews_file_path, books_details_file_path)\n",
        "df = processor.process()\n",
        "\n",
        "# Optional: Use a smaller subset of data if needed\n",
        "# df = df.sample(n=10000, random_state=42)\n",
        "\n",
        "print(\"\\nDataFrame shape:\", df.shape)\n",
        "print(\"\\nColumns:\", df.columns.tolist())\n",
        "print(\"\\nLabel distribution:\")\n",
        "print(df['label'].value_counts())\n",
        "\n",
        "# LSTM model parameters\n",
        "max_words = 5000\n",
        "max_len = 300\n",
        "\n",
        "# Tokenization\n",
        "tokenizer_lstm = Tokenizer(num_words=max_words)\n",
        "tokenizer_lstm.fit_on_texts(df[\"cleaned_review\"])\n",
        "X_lstm = tokenizer_lstm.texts_to_sequences(df[\"cleaned_review\"])\n",
        "X_lstm = pad_sequences(X_lstm, maxlen=max_len)\n",
        "y_lstm = to_categorical(df[\"label\"])\n",
        "\n",
        "# Create LSTM model\n",
        "lstm_model = Sequential()\n",
        "lstm_model.add(Embedding(max_words, 128, input_length=max_len))\n",
        "lstm_model.add(LSTM(64))\n",
        "lstm_model.add(Dropout(0.3))\n",
        "lstm_model.add(Dense(3, activation=\"softmax\"))\n",
        "\n",
        "# Compile model\n",
        "lstm_model.compile(loss=\"categorical_crossentropy\",\n",
        "                  optimizer=\"adam\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "\n",
        "# Print model summary\n",
        "print(\"\\nModel Summary:\")\n",
        "lstm_model.summary()\n",
        "\n",
        "# Train model\n",
        "history = lstm_model.fit(X_lstm, y_lstm,\n",
        "                        batch_size=64,\n",
        "                        epochs=3,\n",
        "                        validation_split=0.2,\n",
        "                        verbose=1)\n",
        "\n",
        "# Print final metrics\n",
        "print(\"\\nTraining completed!\")\n",
        "print(\"Final training accuracy:\", history.history['accuracy'][-1])\n",
        "print(\"Final validation accuracy:\", history.history['val_accuracy'][-1])"
      ],
      "metadata": {
        "id": "WRdlV7D7cwhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import all necessary libraries\n",
        "import pandas as pd\n",
        "import re\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any\n",
        "import swifter\n",
        "from functools import lru_cache\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "\n",
        "# First, define and run the ReviewProcessor\n",
        "class ReviewProcessor:\n",
        "    SENTIMENT_MAPPING = {\n",
        "        1: 0,  # Negative\n",
        "        2: 0,  # Negative\n",
        "        3: 1,  # Neutral\n",
        "        4: 2,  # Positive\n",
        "        5: 2   # Positive\n",
        "    }\n",
        "\n",
        "    def __init__(self, reviews_path: str, books_path: str):\n",
        "        self.reviews_path = Path(reviews_path)\n",
        "        self.books_path = Path(books_path)\n",
        "        self.reviews_df = None\n",
        "        self.books_df = None\n",
        "        self.merged_df = None\n",
        "\n",
        "        # Compile regex patterns once\n",
        "        self.punctuation_pattern = re.compile(r'[^a-zA-Z\\s]')\n",
        "        self.spaces_pattern = re.compile(r'\\s+')\n",
        "\n",
        "        # Verify files exist\n",
        "        if not self.reviews_path.exists():\n",
        "            raise FileNotFoundError(f\"Reviews file not found: {reviews_path}\")\n",
        "        if not self.books_path.exists():\n",
        "            raise FileNotFoundError(f\"Books file not found: {books_path}\")\n",
        "\n",
        "    @lru_cache(maxsize=10000)\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        if pd.isna(text):\n",
        "            return \"\"\n",
        "\n",
        "        text = str(text).lower()\n",
        "        text = self.punctuation_pattern.sub('', text)\n",
        "        text = self.spaces_pattern.sub(' ', text)\n",
        "        return text.strip()\n",
        "\n",
        "    def load_data(self) -> None:\n",
        "        self.reviews_df = pd.read_csv(self.reviews_path)\n",
        "        self.books_df = pd.read_csv(self.books_path)\n",
        "\n",
        "        print(f\"Reviews shape: {self.reviews_df.shape}\")\n",
        "        print(f\"Books shape: {self.books_df.shape}\")\n",
        "\n",
        "    def process_reviews(self) -> None:\n",
        "        if 'reviewText' in self.reviews_df.columns:\n",
        "            review_col = 'reviewText'\n",
        "        else:\n",
        "            text_cols = self.reviews_df.select_dtypes(include=['object']).columns\n",
        "            review_col = text_cols[0] if len(text_cols) > 0 else None\n",
        "\n",
        "        if review_col is None:\n",
        "            raise ValueError(\"Could not find review text column\")\n",
        "\n",
        "        self.reviews_df['cleaned_review'] = (\n",
        "            self.reviews_df[review_col]\n",
        "            .apply(self.clean_text)  # Removed swifter for simplicity\n",
        "        )\n",
        "\n",
        "        # Ensure 'overall' column exists before creating 'label'\n",
        "        if 'overall' in self.reviews_df.columns:\n",
        "            self.reviews_df['label'] = self.reviews_df['overall'].map(self.SENTIMENT_MAPPING)\n",
        "        else:\n",
        "            # Handle the case where 'overall' is missing\n",
        "            print(\"Warning: 'overall' column not found in reviews data. 'label' column will not be created.\")\n",
        "            # You might want to raise an error here if 'label' is strictly required later\n",
        "            # raise KeyError(\"'overall' column required to create 'label' column.\")\n",
        "\n",
        "\n",
        "    def merge_data(self) -> pd.DataFrame:\n",
        "        common_cols = set(self.reviews_df.columns) & set(self.books_df.columns)\n",
        "        if not common_cols:\n",
        "            raise ValueError(\"No common columns found for merging datasets\")\n",
        "\n",
        "        merge_col = list(common_cols)[0]\n",
        "        print(f\"Merging on column: {merge_col}\")\n",
        "\n",
        "        self.merged_df = pd.merge(\n",
        "            self.reviews_df,\n",
        "            self.books_df,\n",
        "            on=merge_col,\n",
        "            how='left'\n",
        "        )\n",
        "\n",
        "        return self.merged_df\n",
        "\n",
        "    def process(self) -> pd.DataFrame:\n",
        "        self.load_data()\n",
        "        self.process_reviews()\n",
        "        return self.merge_data()\n",
        "\n",
        "# Define the file paths\n",
        "reviews_file_path = \"/kaggle/input/amazon-books-reviews/Books_rating.csv\"\n",
        "books_details_file_path = \"/kaggle/input/amazon-books-reviews/books_data.csv\"\n",
        "\n",
        "# Usage\n",
        "processor = ReviewProcessor(reviews_file_path, books_details_file_path)\n",
        "df = processor.process()\n",
        "\n",
        "# Optional: Use a smaller subset of data if needed\n",
        "# df = df.sample(n=10000, random_state=42)\n",
        "\n",
        "print(\"\\nDataFrame shape:\", df.shape)\n",
        "print(\"\\nColumns:\", df.columns.tolist())\n",
        "\n",
        "# Before trying to access df['label'], check if it exists\n",
        "if 'label' in df.columns:\n",
        "    print(\"\\nLabel distribution:\")\n",
        "    print(df['label'].value_counts())\n",
        "else:\n",
        "    print(\"\\n'label' column not found in the processed DataFrame.\")\n",
        "\n",
        "\n",
        "# LSTM model parameters\n",
        "max_words = 5000\n",
        "max_len = 300\n",
        "\n",
        "# Check if 'label' column exists before proceeding with LSTM model\n",
        "if 'label' in df.columns:\n",
        "    # Tokenization\n",
        "    tokenizer_lstm = Tokenizer(num_words=max_words)\n",
        "    tokenizer_lstm.fit_on_texts(df[\"cleaned_review\"])\n",
        "    X_lstm = tokenizer_lstm.texts_to_sequences(df[\"cleaned_review\"])\n",
        "    X_lstm = pad_sequences(X_lstm, maxlen=max_len)\n",
        "    y_lstm = to_categorical(df[\"label\"])\n",
        "\n",
        "    # Create LSTM model\n",
        "    lstm_model = Sequential()\n",
        "    lstm_model.add(Embedding(max_words, 128, input_length=max_len))\n",
        "    lstm_model.add(LSTM(64))\n",
        "    lstm_model.add(Dropout(0.3))\n",
        "    lstm_model.add(Dense(3, activation=\"softmax\"))\n",
        "\n",
        "    # Compile model\n",
        "    lstm_model.compile(loss=\"categorical_crossentropy\",\n",
        "                      optimizer=\"adam\",\n",
        "                      metrics=[\"accuracy\"])\n",
        "\n",
        "    # Print model summary\n",
        "    print(\"\\nModel Summary:\")\n",
        "    lstm_model.summary()\n",
        "\n",
        "    # Train model\n",
        "    history = lstm_model.fit(X_lstm, y_lstm,\n",
        "                            batch_size=64,\n",
        "                            epochs=3,\n",
        "                            validation_split=0.2,\n",
        "                            verbose=1)\n",
        "\n",
        "    # Print final metrics\n",
        "    print(\"\\nTraining completed!\")\n",
        "    print(\"Final training accuracy:\", history.history['accuracy'][-1])\n",
        "    print(\"Final validation accuracy:\", history.history['val_accuracy'][-1])\n",
        "else:\n",
        "    print(\"\\nSkipping LSTM model training as 'label' column is missing.\")"
      ],
      "metadata": {
        "id": "-vAG_GuLhCmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Import all necessary libraries\n",
        "import pandas as pd\n",
        "import re\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any\n",
        "# Ensure swifter is installed and imported if you want to use it later,\n",
        "# but the corrected code removes swifter in the ReviewProcessor.\n",
        "# !pip install swifter\n",
        "# import swifter\n",
        "from functools import lru_cache\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "# Import train_test_split as it's used later in the Bert section\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# First, define and run the ReviewProcessor\n",
        "class ReviewProcessor:\n",
        "    SENTIMENT_MAPPING = {\n",
        "        1: 0,  # Negative\n",
        "        2: 0,  # Negative\n",
        "        3: 1,  # Neutral\n",
        "        4: 2,  # Positive\n",
        "        5: 2   # Positive\n",
        "    }\n",
        "\n",
        "    def __init__(self, reviews_path: str, books_path: str):\n",
        "        self.reviews_path = Path(reviews_path)\n",
        "        self.books_path = Path(books_path)\n",
        "        self.reviews_df = None\n",
        "        self.books_df = None\n",
        "        self.merged_df = None\n",
        "\n",
        "        # Compile regex patterns once\n",
        "        self.punctuation_pattern = re.compile(r'[^a-zA-Z\\s]')\n",
        "        self.spaces_pattern = re.compile(r'\\s+')\n",
        "\n",
        "        # Verify files exist\n",
        "        if not self.reviews_path.exists():\n",
        "            raise FileNotFoundError(f\"Reviews file not found: {reviews_path}\")\n",
        "        if not self.books_path.exists():\n",
        "            raise FileNotFoundError(f\"Books file not found: {books_path}\")\n",
        "\n",
        "    # @lru_cache(maxsize=10000) # lru_cache is not effective with pandas Series apply\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        if pd.isna(text):\n",
        "            return \"\"\n",
        "\n",
        "        text = str(text).lower()\n",
        "        text = self.punctuation_pattern.sub('', text)\n",
        "        text = self.spaces_pattern.sub(' ', text)\n",
        "        return text.strip()\n",
        "\n",
        "    def load_data(self) -> None:\n",
        "        print(\"Loading data...\")\n",
        "        self.reviews_df = pd.read_csv(self.reviews_path)\n",
        "        self.books_df = pd.read_csv(self.books_path)\n",
        "\n",
        "        print(f\"Reviews shape: {self.reviews_df.shape}\")\n",
        "        print(f\"Books shape: {self.books_df.shape}\")\n",
        "        print(\"Reviews columns:\", list(self.reviews_df.columns))\n",
        "        print(\"Books columns:\", list(self.books_df.columns))\n",
        "\n",
        "\n",
        "    def process_reviews(self) -> None:\n",
        "        print(\"Processing reviews...\")\n",
        "        review_col = None\n",
        "        # Explicitly check for 'reviewText' and 'review'\n",
        "        possible_review_cols = ['reviewText', 'review']\n",
        "        for col in possible_review_cols:\n",
        "            if col in self.reviews_df.columns:\n",
        "                review_col = col\n",
        "                break\n",
        "\n",
        "        if review_col is None:\n",
        "            # Fallback to finding the first object/text column if known columns aren't found\n",
        "            text_cols = self.reviews_df.select_dtypes(include=['object', 'string']).columns\n",
        "            if len(text_cols) > 0:\n",
        "                 # Prefer columns with 'review' in the name if multiple text columns exist\n",
        "                 review_col = next((c for c in text_cols if 'review' in c.lower()), text_cols[0])\n",
        "\n",
        "\n",
        "        if review_col is None:\n",
        "             # Raise an error if no suitable review column is found\n",
        "            raise ValueError(f\"Could not find a suitable review text column. Looked for {possible_review_cols} or other text columns.\")\n",
        "        else:\n",
        "            print(f\"Using '{review_col}' as the review text column.\")\n",
        "\n",
        "\n",
        "        # Removed swifter as it was causing issues in the original notebook cell\n",
        "        self.reviews_df['cleaned_review'] = self.reviews_df[review_col].apply(self.clean_text)\n",
        "\n",
        "\n",
        "        # --- MODIFIED SECTION ---\n",
        "        # Check for 'review/score' or 'overall' column for sentiment label\n",
        "        score_col = None\n",
        "        if 'review/score' in self.reviews_df.columns:\n",
        "            score_col = 'review/score'\n",
        "        elif 'overall' in self.reviews_df.columns:\n",
        "            score_col = 'overall'\n",
        "\n",
        "        if score_col is not None:\n",
        "            print(f\"Mapping '{score_col}' column to 'label'...\")\n",
        "            # Convert the score column to numeric, coercing errors to NaN\n",
        "            self.reviews_df[score_col] = pd.to_numeric(self.reviews_df[score_col], errors='coerce')\n",
        "            # Drop rows where the score became NaN after coercion\n",
        "            self.reviews_df.dropna(subset=[score_col], inplace=True)\n",
        "            # Map to sentiment labels\n",
        "            self.reviews_df['label'] = self.reviews_df[score_col].map(self.SENTIMENT_MAPPING)\n",
        "            # Drop rows where label mapping resulted in NaN (e.g., if score had values not in mapping)\n",
        "            self.reviews_df.dropna(subset=['label'], inplace=True)\n",
        "            # Convert label to integer type\n",
        "            self.reviews_df['label'] = self.reviews_df['label'].astype(int)\n",
        "        else:\n",
        "            print(\"Warning: Neither 'review/score' nor 'overall' column found in reviews data. Cannot create 'label' column.\")\n",
        "            # Create an empty 'label' column to prevent later KeyErrors if code expects it\n",
        "            self.reviews_df['label'] = pd.NA # Use pandas NA for nullable integer column\n",
        "\n",
        "\n",
        "        # --- END MODIFIED SECTION ---\n",
        "\n",
        "\n",
        "    def merge_data(self) -> pd.DataFrame:\n",
        "        print(\"Merging data...\")\n",
        "        # Identify common columns for merging\n",
        "        common_cols = set(self.reviews_df.columns) & set(self.books_df.columns)\n",
        "        # Exclude columns that are unlikely merge keys like 'cleaned_review', 'label', 'review/score' etc.\n",
        "        # Be careful excluding too many; 'Id' is used in the data and is a valid merge key here.\n",
        "        # Based on column inspection, 'Id' appears to be the merge key.\n",
        "        merge_cols_to_check = [col for col in common_cols if col not in ['cleaned_review', 'label', 'review/helpfulness', 'review/score', 'review/time', 'review/summary', 'review/text', 'description', 'authors', 'image', 'previewLink', 'publisher', 'publishedDate', 'infoLink', 'categories', 'ratingsCount']]\n",
        "\n",
        "\n",
        "        if not merge_cols_to_check:\n",
        "            # Fallback to checking common columns without strict exclusions if primary keys aren't found\n",
        "            merge_cols_to_check = list(common_cols)\n",
        "            if not merge_cols_to_check:\n",
        "                 raise ValueError(\"No suitable common columns found for merging datasets. Check 'Id' or 'asin' in both files.\")\n",
        "            else:\n",
        "                 print(f\"Falling back to checking all common columns for merge key: {merge_cols_to_check}\")\n",
        "\n",
        "\n",
        "        # Prioritize 'Id' or 'asin' if available (based on typical dataset structures and prior run output)\n",
        "        merge_col = None\n",
        "        if 'Id' in merge_cols_to_check:\n",
        "             merge_col = 'Id'\n",
        "        elif 'asin' in merge_cols_to_check:\n",
        "             merge_col = 'asin'\n",
        "        else:\n",
        "            # Use the first common column if preferred ones aren't found\n",
        "            merge_col = list(merge_cols_to_check)[0]\n",
        "\n",
        "\n",
        "        print(f\"Merging on column: {merge_col}\")\n",
        "\n",
        "        # Ensure merge column is of the same type (e.g., string) before merging\n",
        "        # Coerce errors just in case, though 'Id' looks like it should be fine.\n",
        "        self.reviews_df[merge_col] = self.reviews_df[merge_col].astype(str)\n",
        "        self.books_df[merge_col] = self.books_df[merge_col].astype(str)\n",
        "\n",
        "\n",
        "        self.merged_df = pd.merge(\n",
        "            self.reviews_df,\n",
        "            self.books_df,\n",
        "            on=merge_col,\n",
        "            how='left'\n",
        "        )\n",
        "\n",
        "        self.optimize_memory()\n",
        "        return self.merged_df\n",
        "\n",
        "    def optimize_memory(self) -> None:\n",
        "        print(\"Optimizing memory...\")\n",
        "        for df_name, df in [('reviews_df', self.reviews_df), ('books_df', self.books_df), ('merged_df', self.merged_df)]:\n",
        "            if df is not None:\n",
        "                initial_memory = df.memory_usage(deep=True).sum() / (1024**2)\n",
        "                # print(f\"Initial memory usage for {df_name}: {initial_memory:.2f} MB\") # Keep this commented unless needed for detailed debugging\n",
        "                for col in df.select_dtypes(include=['object', 'string']).columns:\n",
        "                    # Avoid converting columns with very high cardinality or those intended as text like 'cleaned_review'\n",
        "                    if col in ['cleaned_review', 'review/text', 'review/summary']:\n",
        "                         continue\n",
        "\n",
        "                    if df[col].nunique() / len(df) < 0.5: # Adjust threshold if necessary\n",
        "                        # Check for non-numeric values before converting to category if appropriate\n",
        "                        # Or handle NaNs appropriately depending on downstream use\n",
        "                        if not df[col].isnull().any():\n",
        "                            try:\n",
        "                                # Attempt to convert to category\n",
        "                                df[col] = df[col].astype('category')\n",
        "                            except Exception as e:\n",
        "                                # Handle potential issues during conversion\n",
        "                                print(f\"Could not convert non-null column '{col}' to category: {e}\")\n",
        "                        else:\n",
        "                            # If NaNs are present, convert using nullable category type\n",
        "                             try:\n",
        "                                 df[col] = df[col].astype('category')\n",
        "                             except Exception as e:\n",
        "                                 print(f\"Could not convert nullable column '{col}' to category: {e}\")\n",
        "\n",
        "\n",
        "                final_memory = df.memory_usage(deep=True).sum() / (1024**2)\n",
        "                print(f\"Final memory usage for {df_name}: {final_memory:.2f} MB\")\n",
        "\n",
        "\n",
        "    def process(self) -> pd.DataFrame:\n",
        "        self.load_data()\n",
        "        self.process_reviews()\n",
        "        return self.merge_data()\n",
        "\n",
        "\n",
        "# Define the file paths\n",
        "reviews_file_path = \"/kaggle/input/amazon-books-reviews/Books_rating.csv\"\n",
        "books_details_file_path = \"/kaggle/input/amazon-books-reviews/books_data.csv\"\n",
        "\n",
        "# Usage\n",
        "processor = ReviewProcessor(reviews_file_path, books_details_file_path)\n",
        "# You can optionally call inspect_files here to see the columns before processing\n",
        "# processor.inspect_files()\n",
        "result_df = processor.process()\n",
        "\n",
        "# Assign the result to df as the rest of the code expects a dataframe named df\n",
        "df = result_df\n",
        "\n",
        "# Display the first few rows of the result\n",
        "print(\"\\nFirst few rows of processed data:\")\n",
        "# Use display from IPython.display if in a notebook context, otherwise use print\n",
        "try:\n",
        "    from IPython.display import display\n",
        "    display(df.head())\n",
        "except ImportError:\n",
        "    print(df.head())\n",
        "\n",
        "\n",
        "print(\"\\nDataFrame shape:\", df.shape)\n",
        "print(\"\\nColumns:\", df.columns.tolist())\n",
        "\n",
        "# Check if the 'label' column exists before trying to access it\n",
        "if 'label' in df.columns and not df['label'].isnull().all():\n",
        "    print(\"\\nLabel distribution:\")\n",
        "    # Ensure 'label' column has non-null values before value_counts if needed\n",
        "    print(df['label'].value_counts())\n",
        "else:\n",
        "    print(\"Error: 'label' column was not found or contains only null values in the processed DataFrame. Check the presence and format of 'review/score' or 'overall' in the original reviews data.\")\n",
        "\n",
        "# --- Remaining code for LSTM model (assuming 'label' and 'cleaned_review' exist and are suitable) ---\n",
        "# The following code will still fail if 'label' or 'cleaned_review' are not available\n",
        "# or if 'label' contains NaNs after processing.\n",
        "# Add checks here if you want to make this part more robust.\n",
        "\n",
        "if 'cleaned_review' in df.columns and 'label' in df.columns and not df['label'].isnull().all() and len(df['label'].unique()) > 1:\n",
        "    # LSTM model parameters\n",
        "    max_words = 5000\n",
        "    max_len = 300\n",
        "\n",
        "    # Tokenization\n",
        "    print(\"\\nStarting LSTM tokenization...\")\n",
        "    # Drop rows where cleaned_review or label is NaN. Make a copy to avoid SettingWithCopyWarning\n",
        "    df_lstm = df.dropna(subset=['cleaned_review', 'label']).copy()\n",
        "    if df_lstm.empty:\n",
        "        print(\"DataFrame is empty after dropping rows with missing 'cleaned_review' or 'label'. Skipping LSTM.\")\n",
        "    else:\n",
        "        # Ensure label is integer type for to_categorical\n",
        "        df_lstm['label'] = df_lstm['label'].astype(int)\n",
        "\n",
        "        tokenizer_lstm = Tokenizer(num_words=max_words, oov_token=\"<OOV>\") # Added OOV token\n",
        "        tokenizer_lstm.fit_on_texts(df_lstm[\"cleaned_review\"])\n",
        "        X_lstm = tokenizer_lstm.texts_to_sequences(df_lstm[\"cleaned_review\"])\n",
        "        X_lstm = pad_sequences(X_lstm, maxlen=max_len, padding='post', truncating='post') # Added padding/truncating arguments\n",
        "        # Determine number of classes dynamically\n",
        "        num_classes = len(df_lstm['label'].unique())\n",
        "        y_lstm = to_categorical(df_lstm[\"label\"], num_classes=num_classes) # Specify num_classes based on unique labels\n",
        "\n",
        "\n",
        "        # Create LSTM model\n",
        "        print(\"\\nBuilding LSTM model...\")\n",
        "        lstm_model = Sequential()\n",
        "        lstm_model.add(Embedding(max_words, 128, input_length=max_len))\n",
        "        lstm_model.add(LSTM(64))\n",
        "        lstm_model.add(Dropout(0.3))\n",
        "        lstm_model.add(Dense(num_classes, activation=\"softmax\")) # Output layer nodes match number of unique labels\n",
        "\n",
        "        # Compile model\n",
        "        lstm_model.compile(loss=\"categorical_crossentropy\",\n",
        "                          optimizer=\"adam\",\n",
        "                          metrics=[\"accuracy\"])\n",
        "\n",
        "        # Print model summary\n",
        "        print(\"\\nModel Summary:\")\n",
        "        lstm_model.summary()\n",
        "\n",
        "        # Train model\n",
        "        print(\"\\nTraining LSTM model...\")\n",
        "        # Split data before training\n",
        "        X_train_lstm, X_val_lstm, y_train_lstm, y_val_lstm = train_test_split(X_lstm, y_lstm, test_size=0.2, random_state=42, stratify=df_lstm['label']) # Stratify using the label series\n",
        "\n",
        "\n",
        "        history = lstm_model.fit(X_train_lstm, y_train_lstm,\n",
        "                                batch_size=64,\n",
        "                                epochs=3, # Reduced epochs for faster testing if needed\n",
        "                                validation_data=(X_val_lstm, y_val_lstm), # Use dedicated validation data\n",
        "                                verbose=1)\n",
        "\n",
        "        # Print final metrics\n",
        "        print(\"\\nLSTM Training completed!\")\n",
        "        print(\"Final training accuracy:\", history.history['accuracy'][-1])\n",
        "        print(\"Final validation accuracy:\", history.history['val_accuracy'][-1])\n",
        "\n",
        "else:\n",
        "    print(\"\\nSkipping LSTM model training due to missing 'cleaned_review' or 'label' column, all labels are null, or insufficient unique labels (must be > 1).\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "w77twJUGhm05"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
